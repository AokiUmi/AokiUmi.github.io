<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="img/love.jpg"/>
	<link rel="shortcut icon" href="img/love.jpg">
	
			    <title>
    Aoki_Umi
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
 <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <span id="busuanzi_container_site_pv">��վ�ܷ�����<span id="busuanzi_value_site_pv"></span>��</span>
    <meta name="keywords" content="Aoki_Umi" />
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<link rel="alternate" href="atom.xml" title="Aoki_Umi" type="application/atom+xml">
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<script>hljs.initHighlightingOnLoad();</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">AOKI_UMI</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">HOME</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">CATEGORIES</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="../categories/CS/">CS</a></li><li><a class="category-link" href="../categories/CS/AI/">AI</a></li><li><a class="category-link" href="../categories/CS/ALGORITHM/">ALGORITHM</a></li><li><a class="category-link" href="../categories/CS/ASSEMBLY-LANGUAGE/">ASSEMBLY_LANGUAGE</a></li><li><a class="category-link" href="../categories/CS/C-C/">C&C++</a></li><li><a class="category-link" href="../categories/CS/CA/">CA</a></li><li><a class="category-link" href="../categories/CS/PYTHON/">PYTHON</a></li><li><a class="category-link" href="../categories/CS/SIGNAL-PROCESSING/">SIGNAL_PROCESSING</a></li><li><a class="category-link" href="../categories/OI/">OI</a></li><li><a class="category-link" href="../categories/OI/ALGORITHM/">ALGORITHM</a></li><li><a class="category-link" href="../categories/OI/Algorithm/">Algorithm</a></li><li><a class="category-link" href="../categories/OI/Algorithm/Tree/">Tree</a></li><li><a class="category-link" href="../categories/OI/Character-String/">Character String</a></li><li><a class="category-link" href="../categories/OI/Congratulation/">Congratulation</a></li><li><a class="category-link" href="../categories/OI/Contest/">Contest</a></li><li><a class="category-link" href="../categories/OI/DATA-STRUCTURE/">DATA STRUCTURE</a></li><li><a class="category-link" href="../categories/OI/DP/">DP</a></li><li><a class="category-link" href="../categories/OI/GRAPH-THERY/">GRAPH THERY</a></li><li><a class="category-link" href="../categories/OI/Graph-Theory/">Graph Theory</a></li><li><a class="category-link" href="../categories/OI/Maths/">Maths</a></li><li><a class="category-link" href="../categories/OI/Search/">Search</a></li><li><a class="category-link" href="../categories/OI/Simulation/">Simulation</a></li><li><a class="category-link" href="../categories/STUDY/">STUDY</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">ARCHIVES</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="../archives/2023/10/">October 2023</a></li><li><a class="archive-link" href="../archives/2023/09/">September 2023</a></li><li><a class="archive-link" href="../archives/2022/10/">October 2022</a></li><li><a class="archive-link" href="../archives/2022/06/">June 2022</a></li><li><a class="archive-link" href="../archives/2022/01/">January 2022</a></li><li><a class="archive-link" href="../archives/2021/11/">November 2021</a></li><li><a class="archive-link" href="../archives/2021/04/">April 2021</a></li><li><a class="archive-link" href="../archives/2021/01/">January 2021</a></li><li><a class="archive-link" href="../archives/2020/11/">November 2020</a></li><li><a class="archive-link" href="../archives/2020/10/">October 2020</a></li><li><a class="archive-link" href="../archives/2020/08/">August 2020</a></li><li><a class="archive-link" href="../archives/2020/06/">June 2020</a></li><li><a class="archive-link" href="../archives/2020/05/">May 2020</a></li><li><a class="archive-link" href="../archives/2020/04/">April 2020</a></li><li><a class="archive-link" href="../archives/2020/02/">February 2020</a></li><li><a class="archive-link" href="../archives/2018/11/">November 2018</a></li><li><a class="archive-link" href="../archives/2018/10/">October 2018</a></li><li><a class="archive-link" href="../archives/2018/09/">September 2018</a></li><li><a class="archive-link" href="../archives/2018/08/">August 2018</a></li><li><a class="archive-link" href="../archives/2018/07/">July 2018</a></li><li><a class="archive-link" href="../archives/2018/05/">May 2018</a></li><li><a class="archive-link" href="../archives/2018/04/">April 2018</a></li><li><a class="archive-link" href="../archives/2018/03/">March 2018</a></li><li><a class="archive-link" href="../archives/2018/01/">January 2018</a>
	                    </ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="ABOUT">
		                ABOUT
		            </a>
		        </li>
		        
		        <li>
		            <a href="/group/" title="FRIENDS">
		                FRIENDS
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="GALLERY">
		                GALLERY
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="TAG">
		                TAG
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/AokiUmi" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="500px" href="http://500px.com" target="_blank" rel="noopener">
                            <i class="icon fa fa-500px"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(/images/0037.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >AI复习总结</h2></a>
         
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">

                <p>参考有很多，不一一列举，还有很多懒得找了（</p>
<p><a href="https://blog.csdn.net/weixin_43407273/article/details/110856123" target="_blank" rel="noopener">MCMC采样详解_、寄生于黑暗中的光，的博客-CSDN博客_mcmc采样</a></p>
<p><a href="https://www.cnblogs.com/wt869054461/p/9899929.html" target="_blank" rel="noopener">马尔可夫毯（Markov Blanket） - 夕月一弯 - 博客园</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/436214290" target="_blank" rel="noopener">D-separation： 判断贝叶斯网络中的变量是否独立</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/102014899" target="_blank" rel="noopener"> 概率图模型(PGMs)-马尔可夫网络(Markov Nets)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/521329634" target="_blank" rel="noopener">人工智能导论考前复习笔记</a></p>
<h1 id="Search"><a href="#Search" class="headerlink" title="Search"></a>Search</h1><h2 id="树搜索与图搜索"><a href="#树搜索与图搜索" class="headerlink" title="树搜索与图搜索"></a>树搜索与图搜索</h2><p>Tree Search：不设置vis数组，不考虑节点有无被遍历过</p>
<p><strong>搜索树（search trees）</strong> </p>
<p>对于一个状态出现的次数没有限制。</p>
<p>通过移除一个与部分计划对应的节点（用给定的策略来选择）并用它所有的子节点代替它，我们不断地<strong>扩展expand</strong>我们的边缘。用子节点代缘上的元素,相当于丢弃一个长度为n的计划并考虑所有源于它的长度为（n+1）的计划。我们继续这一操作，直到最终将目标从边缘移除为止。</p>
<p>Graph Search：设置vis数组，每次出堆的时候就标记visited，每次只遍历邻接的unvisited的点</p>
<p><strong>图搜索（graph search）</strong></p>
<p>跟踪哪些状态已经扩展过，确保每一个节点在扩展前不在这个集合中，并且在扩展后将其加入集合里。经过这种优化的树搜索称为<strong>图搜索（graph search）</strong></p>
<h2 id="DFS-Depth-First-Search"><a href="#DFS-Depth-First-Search" class="headerlink" title="DFS(Depth-First Search)"></a>DFS(Depth-First Search)</h2><p>深度优先搜索，故名意思一条边走到底再回去，优先搜索最深的边，用stack实现，头进头出</p>
<p>时间复杂度$O(b^m)$ 空间复杂度$O(bm)$</p>
<p>Complete（完备性）：定义是保证能找到答案在答案存在的情况。深度优先搜索并<strong>不具有完备性</strong>。如果在状态空间图中存在回路，这必然意味着相应搜索树的深度将是无限的。因此，存在这样一种可能性，即DFS老实地在无限大的搜索树中搜索最深的节点而不幸地陷入僵局，注定无法找到解。</p>
<p>Optimal：显然不是最优</p>
<p><img src="/2022/10/29/AI复习总结/1.png" alt=""></p>
<h2 id="BFS-Breadth-First-Search"><a href="#BFS-Breadth-First-Search" class="headerlink" title="BFS(Breadth-First Search)"></a>BFS(Breadth-First Search)</h2><p>广度优先搜索，优先最浅的边，用queue实现，头进尾出</p>
<p>时间复杂度$O(b^s)$，空间复杂度$O(b^s)$，Shallowest点的深度为s</p>
<p>Complete：是完整的搜索</p>
<p>Optimal：只有cost都为1才能找到最优解</p>
<p><img src="/2022/10/29/AI复习总结/2.png" alt=""></p>
<h2 id="Iterative-Deepening-迭代加深的深度优先搜索"><a href="#Iterative-Deepening-迭代加深的深度优先搜索" class="headerlink" title="Iterative Deepening(迭代加深的深度优先搜索)"></a>Iterative Deepening(迭代加深的深度优先搜索)</h2><p><strong>Iterative Deepening</strong></p>
<p><img src="/2022/10/29/AI复习总结/3.png" alt=""></p>
<p>从深度为0开始，深度不断增大，直到找到目标</p>
<p>Complete：Yes</p>
<p>Optimal：Yes</p>
<p>时间复杂度$ O(b^d)$,空间$O(bd)$<strong>比BFS空间更优</strong></p>
<h2 id="UCS-Uniform-Cost-Search"><a href="#UCS-Uniform-Cost-Search" class="headerlink" title="UCS(Uniform Cost Search)"></a>UCS(Uniform Cost Search)</h2><p>优先搜索最便宜的点路径，用的是优先队列</p>
<p>If that solution costs $C^<em>$  and arcs cost(两点间的最小代价)  $\epsilon$  , then the“effective depth” is roughly $C^</em>/\epsilon$</p>
<p>时间复杂度$O(C^*/\epsilon)$(exponential in effective depth)</p>
<p>空间复杂度$O(C^*/\epsilon)$</p>
<p>Complete：Yes</p>
<p>Optimal：Yes</p>
<p><img src="/2022/10/29/AI复习总结/4.png" alt=""></p>
<h2 id="GS-Greedy-Search"><a href="#GS-Greedy-Search" class="headerlink" title="GS(Greedy Search)"></a>GS(Greedy Search)</h2><p>Heuristics函数，估计期望函数一般表示到重点的距离估计</p>
<p>贪心直往离终点最近的路走，必然不优。</p>
<p>Complete：Yes</p>
<p>Optimal：No</p>
<p>如果存在一个目标状态，贪婪搜索<strong>无法保证能找到它</strong>，它也<strong>不是最优的</strong>，尤其是在选择了非常糟糕的启发函数的情况下。在不同场景中。它的行为通常是不可预测的，有可能一路直奔目标状态，也有可能像一个被错误引导的DFS一样并遍历所有的错误区域。</p>
<h2 id="A-Search"><a href="#A-Search" class="headerlink" title="A* Search"></a>A* Search</h2><p>$f(n)=g(n)+h(n)$</p>
<p>结合贪心和UCS，按该点的h+该点到起点的cost</p>
<p>H函数的性质：①admissible：$0\leq h(n)\leq h^*(n)$，小于实际n到终点的距离。</p>
<p>②Consistency：$h(A)-h(C)\leq cost(A,C)$，两个点H只差小于真实距离，<strong>满足此性质即可满足最优解</strong></p>
<p>Consistency $\implies$ admissibility</p>
<p>A* is optimal if the heuristic is <strong>admissible</strong> in the <strong>tree search</strong>.</p>
<h1 id="CSP-Constraint-Satisfaction-Problems"><a href="#CSP-Constraint-Satisfaction-Problems" class="headerlink" title="CSP(Constraint Satisfaction Problems)"></a>CSP(Constraint Satisfaction Problems)</h1><p>Constraint Graphs：图上相连的点表示这两个点之间有限制关系</p>
<h2 id="搜索方法："><a href="#搜索方法：" class="headerlink" title="搜索方法："></a>搜索方法：</h2><p>Backtracking Search：总的来说就是DFS+回溯算法，把所有可以取的值都取试一遍，可以继续就不断往下搜索，遇到问题就返回，不断撤回取值换一个值知道找到解。很慢，会走很多弯路，浪费时间。</p>
<p><img src="/2022/10/29/AI复习总结/5.png" alt=""></p>
<h3 id="改进过程"><a href="#改进过程" class="headerlink" title="改进过程"></a>改进过程</h3><h3 id="Filtering"><a href="#Filtering" class="headerlink" title="Filtering"></a>Filtering</h3><p>每次取值的时候把不能用的过滤掉，减少取值的domain</p>
<p>①Forward checking：每次划掉与当前取值变量有关联的变量取值，缺点是不能提前发现错误，比如在第三步的时候NT和SA都只能取蓝色其实已经矛盾了，因为每次只检查了与现在在填的格子相邻的格子取值，没有单独考虑相邻格子和其限制点的取值比较。</p>
<p><img src="/2022/10/29/AI复习总结/6.png" alt=""></p>
<p>②Constraint Propagation：加入了每次将每个点都和其他相关的限制点domain的比较，加入consitent定义来检查每个点的限制点取值比较</p>
<p>An arc X → Y is consistent iff for every x in the tail there is some y in the head which could be assigned without violating a constraint</p>
<p>A simple form of propagation makes sure all arcs are consistent，每次propagation确保每个点都能满足consistent条件</p>
<p><img src="/2022/10/29/AI复习总结/7.png" alt=""></p>
<p>这里其实我觉得NSW也可以取蓝色，V变成红绿，这里表现能提前排除错误</p>
<p>K-Consistency：弧一致性是一个更广义的一致性概念——<strong>k-相容（k-consistency）</strong> 的子集，它在强制执行时能保证对于CSP中的任意k个节点，对于其中任意k-1个节点组成的子集的赋值都能保证第k个节点能至少有一个满足相容的赋值。这个想法可以通过<strong>强k-相容（strong k-consistency）</strong> 的思想进行进一步拓展。一个具有强k-相容的图拥有这样的性质，任意k个节点的集合不仅是k-相容的，也是k-1, k-2, …,1-相容的。于是，在CSP中更高阶的相容计算的代价也更高。在这一广义的相容的定义下，我们可以看出弧相容等价于2-相容。</p>
<h3 id="Ordering"><a href="#Ordering" class="headerlink" title="Ordering"></a>Ordering</h3><p>①<strong>最小剩余值Minimum Remaining Values (MRV)</strong>：当选择下一个要赋值的<strong>变量</strong>时，用MRV策略能选择有效剩余值最少的未赋值变量（即约束最多的变量）。这很直观，因为受到约束最多的变量也最容易耗尽所有可能的值，并且如果没有赋值的话最终会回溯，所以最好尽早赋值。</p>
<p>这个是针对赋值的variable的选择</p>
<p>②<strong>最少约束值Least Constraining Value (LCV)</strong>：同理，当选择下一个要赋值的<strong>值</strong>时，一个好的策略就是选择从剩余未分配值的域中淘汰掉最少值的那一个。要注意的是，这要求更多的计算（比方说，对每个值重新运行弧相容/前向检测或是其他过滤方法来找到其LCV），但是取决于用途，仍然能获得更快的速度。</p>
<p>这个是针对对每个variable的value如何选择，尽量不要造成更多约束，选择约束最少的value，如下图应选择红色而不是蓝色</p>
<p><img src="/2022/10/29/AI复习总结/8.png" alt=""></p>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><h3 id="Tree-Structured-CSPs"><a href="#Tree-Structured-CSPs" class="headerlink" title="Tree-Structured CSPs"></a>Tree-Structured CSPs</h3><p><img src="/2022/10/29/AI复习总结/9.png" alt=""></p>
<p>Theorem: if the constraint graph has no loops, the CSP can be solved in $O(n d^2 )$ time</p>
<ul>
<li>首先，在CSP的约束图中任选一个节点来作为树的根节点（具体选哪一个并不重要，因为基础图论告诉我们一棵树的任一节点都可以作为根节点）。</li>
<li>将树中的所有无向边转换为指向根节点反方向的有向边。然后将得到的有向无环图<strong>线性化（linearize）</strong>（或<strong>拓扑排序（topologically sort）</strong>）。简单来说，这也就意味着将图的节点排序，让所有边都指向右侧。注意我们选择节点A作为根节点并让所有边都指向A的反方向，这一过程的结果是如下的CSP转换：</li>
</ul>
<p>①Remove backward: For i = n : 2, apply RemoveInconsistent(Parent(Xi ),Xi )，从孩子往父亲搜索，剪掉父亲不能consistent的值</p>
<p>②Assign forward: For i = 1 : n, assign Xi consistently with Parent(Xi )</p>
<p>从父亲开始往孩子赋值，在孩子中选取和父亲consistent的值</p>
<h3 id="Nearly-Tree-Structured-CSPs"><a href="#Nearly-Tree-Structured-CSPs" class="headerlink" title="Nearly Tree-Structured CSPs"></a>Nearly Tree-Structured CSPs</h3><p>通过<strong>割集条件设置（cutset conditioning）</strong>，树形结构算法能推广到树形结构相近的的CSP中。割集条件设置包括首先找到一个约束图中变量的最小子集，这样一来删去他们就能得到一棵树（这样的子集称为<strong>割集（cutset）</strong>）。举个例子，在我们的地图染色例题中，South Australia（SA）是可能的最小的割集：</p>
<p><img src="/2022/10/29/AI复习总结/10.png" alt=""></p>
<p>意思大概是通过不断的割掉点来减少图的边数和点数吧</p>
<h1 id="Adversarial-Search"><a href="#Adversarial-Search" class="headerlink" title="Adversarial Search"></a>Adversarial Search</h1><h2 id="Minimax-Values"><a href="#Minimax-Values" class="headerlink" title="Minimax Values"></a>Minimax Values</h2><p>基本内容就是一个人求最大，另一个人为了妨碍这个人只求最小</p>
<p>由于是本质还是DFS，所以复杂度是$O(b^m)$</p>
<p><img src="/2022/10/29/AI复习总结/11.png" alt=""></p>
<h2 id="α-β剪枝-Alpha-Beta-Pruning"><a href="#α-β剪枝-Alpha-Beta-Pruning" class="headerlink" title="α-β剪枝 Alpha-Beta Pruning"></a>α-β剪枝 Alpha-Beta Pruning</h2><p><img src="/2022/10/29/AI复习总结/12.png" alt=""></p>
<p>简单来说就是对于一个求最大\最小值的父亲节点的子节点，前面已经求出一个孩子的值，由于父亲只会返回最大\最下的孩子节点的值，如果某一个孩子的某一个节点已经相对于前一个孩子的值对父亲没有贡献了，就可以不搜索这个子树了，剪枝掉。</p>
<p>例子：下图f分支已经有b节点的10，由于a求最小，而f的100大于了10，并且e节点本身是要选最大值的，已经找出一个比较大的可能答案且比10还大，说明a节点不可能选e，故g要被剪。同理对于h节点，i这个还是是2且2已经小于了10，h本身求得的最小值已经小于a节点，根节点就不可能选h了所以l就被剪掉了。</p>
<p><img src="/2022/10/29/AI复习总结/13.png" alt=""></p>
<p>时间复杂度：$O(b^{m/2})$</p>
<p><strong>讨论一下什么节点的孩子是可能会被剪掉的</strong></p>
<p>①子树节点的最左边的子树一定不会被剪掉，无论如何都要比较第一个值，所以最左边最左下角的子树都要全部遍历（cd,jk)，同时所有没有被剪得节点的左边第一个孩子一定会遍历，比如上图中的f</p>
<p>②能被剪掉的一定是自己的siblings已经得出了部分答案，自己在被比较的过程中才会因为前一个siblings得出的答案无效所以中止检索。</p>
<h2 id="估计函数-Evaluation-Function"><a href="#估计函数-Evaluation-Function" class="headerlink" title="估计函数 Evaluation Function"></a>估计函数 Evaluation Function</h2><p>一个好的估计函数能给更好的状态赋更高的值。估计函数在<strong>深度限制（depth-limited）minimax</strong>中广泛应用，即将最大可解深度处的非叶子结点都视作叶子节点，然后用仔细挑选的估计函数给其赋虚值。由于估计函数只能用于估计非叶子结点的效益，这使得minimax的最优性不再得到保证。</p>
<p>$Eval(s)=w_1f_1(s)+w_2f_2(s)+ …+w_nf_n(s)$</p>
<p> $f(s)$ 对应从状态s中提取的一个特征，每个特征被赋予了一个相应的权重  。特征就是游戏状态中能够提取并量化的一些元素。</p>
<h2 id="Expectimax-Search"><a href="#Expectimax-Search" class="headerlink" title="Expectimax Search"></a>Expectimax Search</h2><p>Expectimax在游戏树中加入了<strong>机会节点（chance nodes）</strong>，与考虑最坏情况的最小化节点不同，机会节点会考虑<strong>平均情况（average case）</strong>。更准确的说，最小化节点仅仅计算子节点的最小效益，而机会节点计算<strong>期望效益（expected utility）</strong> 或期望值。Expectimax给节点赋值的规则如下：</p>
<p><img src="/2022/10/29/AI复习总结/15.png" alt=""></p>
<p>其中，$p(s’|s)$  要么表示在不能确定操作的情况下从状态s移动到s’的概率，要么表示对手的操作使得最终从s移动到s’的概率，这取决于游戏和游戏树的特性。从这个定义来看，minimax就是expectimax的一种特例，最小化节点就是认为值最小的子节点概率为1而其他子节点概率为0的机会节点。</p>
<p>总的来说，概率是用来合理反映游戏状态的，但我们会在后面的笔记中更详细地介绍这一过程的原理。现在，我们可以吧这些概率看做游戏自带的特性。</p>
<p>Expectimax的伪代码与minimax很相似，就是把最小效益换成了期望效益，这是因为最小化节点被替换成了机会节点：</p>
<p><img src="/2022/10/29/AI复习总结/14.png" alt=""></p>
<p>关于Expectimax，最后再说一句，需要重点注意的是，必须要遍历机会节点的所有子节点——不能再像minimax中一样进行剪枝。与minimax中求最大值或最小值时不同，每个值都会影响expectimax计算的期望值。不过，当我们已知节点有限的取值范围时，剪枝也是有可能的。</p>
<h1 id="Propositional-Logic"><a href="#Propositional-Logic" class="headerlink" title="Propositional Logic"></a>Propositional Logic</h1><p><img src="/2022/10/29/AI复习总结/16.png" alt=""></p>
<h2 id="Conjunctive-Normal-Form-CNF"><a href="#Conjunctive-Normal-Form-CNF" class="headerlink" title="Conjunctive Normal Form (CNF)"></a>Conjunctive Normal Form (CNF)</h2><p>conjunction of disjunctions of literals (clauses)</p>
<p>基本形式是()里面都是∪，外面套∩</p>
<h2 id="Horn-logic"><a href="#Horn-logic" class="headerlink" title="Horn logic"></a>Horn logic</h2><p>Horn logic: only (strict) Horn clauses are allowed</p>
<p>– A Horn clause has the form:</p>
<p>$P_1 \bigcap P_2 \bigcap …P_n \implies Q$</p>
<p>or alternatively</p>
<p>$ \neg P_1 \bigcup \neg  P_2 \bigcup … \neg P_n \bigcup Q$</p>
<p>where Ps and Q are non-negated proposition symbols</p>
<h2 id="经典例题"><a href="#经典例题" class="headerlink" title="经典例题"></a>经典例题</h2><p>$[C \bigcup (\neg A \bigcap \neg B)] = [(\neg A \bigcap \neg B)\bigcup (C \bigcap \neg B) \bigcup ( \neg A \bigcap C) \bigcup C]$</p>
<p>$(\neg A \bigcup \neg B)=((A \bigcap B) \rightarrow (A \bigcap \neg A))$</p>
<h2 id="Validity-and-satisfiability"><a href="#Validity-and-satisfiability" class="headerlink" title="Validity and satisfiability"></a>Validity and satisfiability</h2><p>A sentence is valid if it is true in all models</p>
<p>A sentence is satisfiable if it is true in some model</p>
<p>A sentence is unsatisfiable if it is true in no models</p>
<p>S is valid iff. $\neg $S is unsatisfiable</p>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><h3 id="entails"><a href="#entails" class="headerlink" title="entails"></a>entails</h3><p>$ A| !!!\ = B$ A entails B,相当于$A \implies B$</p>
<p>A is valid if and only if True entails A</p>
<h3 id="proof"><a href="#proof" class="headerlink" title="proof"></a>proof</h3><p>$A | !!!- B$ a demonstration of entailment from A to B</p>
<p>Method 1: model checking</p>
<p>Method 2: application of inference rules</p>
<p><strong>Sound inference</strong>：everything that can be proved is in fact entailed</p>
<p><strong>Complete inference</strong>：everything that is entailed can be proved</p>
<p>Resolution algorithm：Prove$KB |!!!= \alpha$ Proof by contradiction,show $KB |!!!= \alpha$ is unsatisfiable</p>
<ul>
<li><p>把$KB  \bigcap \neg \alpha$转换为CNF</p>
</li>
<li><p>. Repeatedly apply the resolution rule to add new clauses, until one of the two things happens</p>
<p>a) Two clauses resolve to yield the empty clause, in which</p>
<p>case$KB |!!! = \alpha $</p>
<p>b) There is no new clause that can be added, in which case</p>
<p>KB does not entail α</p>
</li>
</ul>
<h2 id="Forward-chaining"><a href="#Forward-chaining" class="headerlink" title="Forward chaining"></a>Forward chaining</h2><p>从 implies 前面的条件开始往implies后面的值推导</p>
<p><img src="/2022/10/29/AI复习总结/17.png" alt=""></p>
<p>从A B L M  P Q ，没往上推 节点的未知数就-1，直到都为0</p>
<h2 id="Backward-chaining"><a href="#Backward-chaining" class="headerlink" title="Backward chaining"></a>Backward chaining</h2><p>从果找因</p>
<p>• Idea: work backwards from the query q:</p>
<p>– to prove q by BC</p>
<p>• check if q is known to be true already, or prove by BC all premises of some rule concluding q</p>
<p>• Avoid loops: check if new subgoal is already on the goal</p>
<p>stack</p>
<p>• Avoid repeated work: check if new subgoal</p>
<ol>
<li><p>has already been proved true, or</p>
</li>
<li><p>has already failed</p>
</li>
</ol>
<h1 id="First-Order-Logic"><a href="#First-Order-Logic" class="headerlink" title="First-Order Logic"></a>First-Order Logic</h1><h2 id="FOL"><a href="#FOL" class="headerlink" title="FOL"></a>FOL</h2><p><img src="/2022/10/29/AI复习总结/18.png" alt=""></p>
<p><strong>In a FOL sentence, every variable must be bound.</strong></p>
<p>判断是否为FOL句式，要看变量有没有被量词限定</p>
<h2 id="Atomic-sentences"><a href="#Atomic-sentences" class="headerlink" title="Atomic sentences"></a>Atomic sentences</h2><p><img src="/2022/10/29/AI复习总结/19.png" alt=""></p>
<p>复杂句子由很多原子句子组成</p>
<ul>
<li>任意句式：$\forall x, At(x,stu) \implies Smart(x)$(Everyone at ShanghaiTech is smart)</li>
</ul>
<p>一般任意都接imply，不要用并</p>
<ul>
<li>存在句式：$\exist x,At(x,stu) \bigcup Smart(x)$ (Someone at ShanghaiTech is smart)</li>
</ul>
<p>存在句式用并，不要用imply</p>
<p><strong>任意和存在量词的一些性质</strong></p>
<p><img src="/2022/10/29/AI复习总结/20.png" alt=""></p>
<h2 id="两个变量替换原则"><a href="#两个变量替换原则" class="headerlink" title="两个变量替换原则"></a>两个变量替换原则</h2><ul>
<li>Universal instantiation (UI)</li>
</ul>
<p>全称列举规则，UI 规则的实质，可以看成是包含量词的符号式的展开式，通过简化式推论规则，缩略为其中某个“合取项”。</p>
<p>大概是变量代入具体的意思（），变量替换</p>
<ul>
<li>Existential Instantiation(EI)</li>
</ul>
<p>存在列举规则</p>
<p>存在列举规则，就是摘除存在量词。</p>
<p>与 UI 不同， EI 规则摘除量词后，该行中的自由变量不是全称，而是被制定出来用以指派一个具体个体的名称。</p>
<p>请注意，这个个体不是被任意选出的。</p>
<h2 id="Unification"><a href="#Unification" class="headerlink" title="Unification"></a>Unification</h2><p>好像就是找出一个变量替换的方式。。。如下图找到x,y分别代表什么</p>
<p><img src="/2022/10/29/AI复习总结/21.png" alt=""></p>
<h2 id="Forward-chaining-1"><a href="#Forward-chaining-1" class="headerlink" title="Forward chaining"></a>Forward chaining</h2><p>和之前那个Forward chaining差不多，从implies前的推到implies后面的</p>
<p>性质：</p>
<ul>
<li><p>Sound and complete for first-order Horn clauses</p>
</li>
<li><p>FC terminates for first-order Horn clauses with no functions (Datalog) in finite number of iterations</p>
</li>
<li><p>In general, FC may not terminate if α is not entailed，只要没找到不会中止</p>
</li>
</ul>
<h2 id="Backward-chaining-1"><a href="#Backward-chaining-1" class="headerlink" title="Backward chaining"></a>Backward chaining</h2><p>和之前那个Back chaining差不多，从implies后的找到implies前面的，由果及因</p>
<p>性质：</p>
<ul>
<li><p>Depth-first recursive proof search: space is linear in size of proof线性证明</p>
</li>
<li><p>Avoid infinite loops by checking current goal against every goal on stack有效避免无限循环</p>
</li>
<li><p>Avoid repeated subgoals by caching previous results不会重复找子问题</p>
</li>
</ul>
<p><strong>FC and BC are sound and complete with Horn clauses and run linear in space and time.</strong></p>
<h1 id="Bayes-Network"><a href="#Bayes-Network" class="headerlink" title="Bayes Network"></a>Bayes Network</h1><h2 id="条件独立"><a href="#条件独立" class="headerlink" title="条件独立"></a>条件独立</h2><p>$X \perp !!! \perp Y|Z$ : X is conditionally independent of Y given Z</p>
<p>$X \perp !!! \perp Y|Z \Leftrightarrow P(x|y,z)=P(x|z),P(x,y|z)=P(x|z)P(y|z)$</p>
<p>贝叶斯网络是一种描述<strong>随机变量之间互相条件独立关系的有向无环图</strong>。在这个有向无环图中，每个节点代表一个<strong>随机变量对其父节点的条件概率分布</strong> $P(X_i|parents(X_i)$ ，每一条边可以理解成变量之间的联系。</p>
<p>A Bayes net = Topology (graph) + Local Conditional Probabilities</p>
<p>贝叶斯网络的空间：n变量，最大取值空间d，最大父节点数量k，满节点的分布$O(d^n)$贝叶斯网络大小$O(nd^{k+1})$</p>
<p>贝叶斯网络是有向无环图，$B \rightarrow A$ 代表B是A的条件，有公式</p>
<p>$P(X_1,…,X_n)=\prod_{i}P(X_i|Parents(X_i)$</p>
<p><img src="/2022/10/29/AI复习总结/22.png" alt=""></p>
<p>When Bayes nets reflect the true causal patterns:</p>
<p>▪ Often simpler (fewer parents, fewer parameters)</p>
<p>▪ Often easier to assess probabilities</p>
<p>▪ Often more robust: e.g., changes in frequency of burglaries should not affect the rest of the model</p>
<p>BNs need not actually be causal</p>
<p><strong>BN don’t need to reflect the true causal patterns</strong></p>
<h3 id="给定贝叶斯网络上部分点，判断两点是否独立："><a href="#给定贝叶斯网络上部分点，判断两点是否独立：" class="headerlink" title="给定贝叶斯网络上部分点，判断两点是否独立："></a>给定贝叶斯网络上部分点，判断两点是否独立：</h3><p><strong>[Step 1]. Draw the ancestral graph.</strong></p>
<p>根据原始概率图，构建包括表达式中包含的变量以及这些变量的ancestor节点（父节点、父节点的父节点…）的图。<strong>注意不能包括孩子节点，不然会引起错误。</strong></p>
<p><strong>[Step 2]. “Moralize” the ancestral graph by “marrying” the parents.</strong></p>
<p>连接图中每个collider结构中的父节点，即若两个节点有同一个子节点，则连接这两个节点。（若一个变量的节点有多个父节点，则分别链接每一对父节点）。</p>
<p><strong>[Step 3]. “Disorient” the graph by replacing the directed edges (arrows) with undirected edges (lines).</strong></p>
<p>去掉图中所有的路径方向，将directional graph变为non-directional graph。</p>
<p><strong>[Step 4]. Delete the givens and their edges.</strong></p>
<p>从图中删除需要判断的概率表达式中作为条件的变量，以及和他们相连的路径。比如“是否P(A|BDF) = P(A|DF)?”，我们删掉D, F变量以及他们的路径。</p>
<p><strong>[Step 5]. Read the answer off the graph.</strong></p>
<ul>
<li>如果变量之间没有连接，则它们在给定条件下是独立的；</li>
<li>如果变量之间有路径连接，则它们不能保证是独立的（或者粗略地说他们是不独立的，基于概率图来说）；</li>
<li>如果其中一个变量或者两者都不包含在现在的图中（作为观测条件，在step 4 被删掉了），那么他们是独立的。</li>
</ul>
<p>补充概念：活跃与非活跃道路</p>
<p><img src="/2022/10/29/AI复习总结/23.png" alt=""></p>
<h2 id="马尔可夫网络-Markov-Networks"><a href="#马尔可夫网络-Markov-Networks" class="headerlink" title="马尔可夫网络(Markov Networks)"></a>马尔可夫网络(Markov Networks)</h2><p>是一个无向图</p>
<p>Markov network = undirected graph + potential functions</p>
<h3 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h3><p><img src="/2022/10/29/AI复习总结/24.png" alt=""></p>
<p>有一个特定的rule$P(X_1,X_2,…,X_t)=P(X_1)\prod_{t=2}^TP(X_t|X_{t-1})$</p>
<p>只需要初始概率和条件概率表（CPT）就可以得出任何一个时段，某一件事所要发生的概率。如果一个马尔可夫链接近无限长，最终概率会越来越接近一个平稳大小。</p>
<p>这个我们称作，静态分布（Stationary Distributions)。每一个马尔可夫链都至少有一个静态分布。对于一些特殊结构的马尔可夫链，可能只有唯一一个静态分布。比如：不可还原的（Irreducible）马尔可夫链或者叫正常的（Regular)马尔可夫链</p>
<p>表示的是马尔可夫链上的点可以有机会到达其它任何一个点，就表示它是Irreducible。</p>
<p>当一个马尔可夫链是irreducible的，那么它一定会有唯一一个静止状态。</p>
<p>但要想确定它是否可能最终收敛到这个静止状态，还需要另外一个属性来判断，看他是不是非周期性的（Aperiodic）</p>
<h3 id="隐马尔可夫模型-HMM"><a href="#隐马尔可夫模型-HMM" class="headerlink" title="隐马尔可夫模型(HMM)"></a>隐马尔可夫模型(HMM)</h3><p>马尔可夫链的基础上加了一层可观察层，原来的链路变成了不可观察：</p>
<p><img src="/2022/10/29/AI复习总结/25.png" alt=""></p>
<p>$P(X_1,E_1,…,X_T,E_T)=P(X_1)P(E_1|X_1)\prod_{t=2}^TP(X_t|X_{t-1})P(E_t|X_t)$</p>
<p>性质：</p>
<ol>
<li>在已知现在(present)的可观察的状态下，未来(future)的可观察的状态，是独立于过去(past)的可观察的状态。</li>
<li>在已知所有的可观察的状态下，可观察的状态与可观察状态之间是独立的。</li>
</ol>
<h2 id="马尔可夫网络-Markov-Nets"><a href="#马尔可夫网络-Markov-Nets" class="headerlink" title="马尔可夫网络(Markov Nets)"></a>马尔可夫网络(Markov Nets)</h2><p>对于马尔可夫网络中，我们也可以用势能 （$\phi_i$ ）来表示不同结点间影响力的大小或者结点团之间影响力的大小。其实这个概念是源自物理中的势能。</p>
<p>我们可以通过一个被称作联合分布函数的来表示这个马尔可夫网络（也被称作吉布斯分布/Gibbis分布）</p>
<p>引入一个clique（小集团）的定义，用这个来定义分布</p>
<p>$P(X)=\frac{1}{Z}\prod_{c\in cliques(H)}\phi_c(x_c)$</p>
<p>$Z=\sum_x \prod_{c \in cliques(H)}\phi_c(x_c)$</p>
<p><img src="/2022/10/29/AI复习总结/27.png" alt=""></p>
<p><img src="/2022/10/29/AI复习总结/26.png" alt=""></p>
<p>$P(X)=\frac{1}{Z}\phi_1(A,B)\phi_2(B,C)\phi_3(C,D)\phi_4(D,A)$</p>
<h2 id="马尔可夫毯"><a href="#马尔可夫毯" class="headerlink" title="马尔可夫毯"></a>马尔可夫毯</h2><p>简单来说就是T的父亲节点+T的孩子+T的孩子的父亲</p>
<p><img src="/2022/10/29/AI复习总结/28.png" alt=""></p>
<h3 id="贝叶斯网络和马尔可夫网络的转换"><a href="#贝叶斯网络和马尔可夫网络的转换" class="headerlink" title="贝叶斯网络和马尔可夫网络的转换"></a>贝叶斯网络和马尔可夫网络的转换</h3><p>Steps</p>
<ol>
<li><p>Moralization</p>
</li>
<li><p>Construct potential functions from CPTs</p>
</li>
</ol>
<p>The BN and MN encode the same distribution，两个表示同一个分布</p>
<p>But they don’t encode the same set of conditional independence但表示的独立性不同</p>
<p>①链式结构，直接转换</p>
<p><img src="/2022/10/29/AI复习总结/29.png" alt=""></p>
<p>②非链式结构，要加边</p>
<p>只要有共同的父亲就要练边，每个父亲两两相连</p>
<p><img src="/2022/10/29/AI复习总结/30.png" alt=""></p>
<h1 id="Bayes-Nets-Exact-Inference"><a href="#Bayes-Nets-Exact-Inference" class="headerlink" title="Bayes Nets: Exact Inference"></a>Bayes Nets: Exact Inference</h1><p>calculating some useful quantity from a probabilistic model (joint probability distribution)</p>
<h2 id="Join-factors"><a href="#Join-factors" class="headerlink" title="Join factors"></a>Join factors</h2><p>字面意思就是把条件放到条件前面去$P(r) * P(t|r) \rightarrow P(r,t)$</p>
<p>需要一直单独的概率和条件概率</p>
<p><img src="/2022/10/29/AI复习总结/31.png" alt=""></p>
<h2 id="Eliminate-factors"><a href="#Eliminate-factors" class="headerlink" title="Eliminate factors"></a>Eliminate factors</h2><p>直接把某个变量删掉，必须知道联合分布</p>
<p>一般采用enumeration 的顺序，即从条件到非条件，沿着箭头来，但也可以不按这个顺序来。</p>
<p>每次eliminate一个变量就相当于把他哦才能够贝叶斯网络里面划掉，只用eliminate目标变量的祖先节点，不用管孩子节点。</p>
<p><img src="/2022/10/29/AI复习总结/32.png" alt=""></p>
<h1 id="Bayes-Nets-Approximate-Inference"><a href="#Bayes-Nets-Approximate-Inference" class="headerlink" title="Bayes Nets: Approximate Inference"></a>Bayes Nets: Approximate Inference</h1><h2 id="Prior-Sampling-直接采样"><a href="#Prior-Sampling-直接采样" class="headerlink" title="Prior Sampling (直接采样)"></a>Prior Sampling (直接采样)</h2><p>生成全联合概率分布，这里可以理解为路上的随机采访，因为每个节点的值概率是不一样的，所以在每个节点采样值和概率相关，我们就是通过直接采样，就像街上的直接采访一样，一个一个采样，最后生成一个概率分布表。</p>
<p>直接按原图的顺序一个个采就行</p>
<p><img src="/2022/10/29/AI复习总结/33.png" alt=""></p>
<h2 id="Rejection-Sampling-拒绝采样"><a href="#Rejection-Sampling-拒绝采样" class="headerlink" title="Rejection Sampling(拒绝采样)"></a>Rejection Sampling(拒绝采样)</h2><p>在直接采样的基础上删去不符合的样本</p>
<p>对于含有随机接受度的拒绝采样，如果采样被拒绝了即取负值，则在下一个接受度重新取直到是正的再继续取别的。只要被拒绝就重新采样。</p>
<p><img src="/2022/10/29/AI复习总结/34.png" alt=""></p>
<h2 id="Likelihood-Weighting-似然加权"><a href="#Likelihood-Weighting-似然加权" class="headerlink" title="Likelihood Weighting(似然加权)"></a>Likelihood Weighting(似然加权)</h2><p>对<strong>证据变量不采样</strong>，直接利用条件分布表的概率，给每个样本一个权重，也称作似然。</p>
<p>我的理解就是weight就是证据变量的条件概率乘积下图的意思就是采样S,W是证据变量，其他都要有一个采样这里sample是c,s,r,w，c,r都是采样的</p>
<p>所以该样本weight就等于$P(S|C)P(W|S,R)$，这个1.0指的是weight一开始就是1.0给我干寂寞了之前一直没看懂</p>
<p>weight是evidence的$e_i,\prod_i^mP(e_i|Parents(e_i))$</p>
<p><img src="/2022/10/29/AI复习总结/35.png" alt=""></p>
<p>下图写的就是这个过程</p>
<p><img src="/2022/10/29/AI复习总结/36.png" alt=""></p>
<p>下面就是计算完所有权值之后再来算给定的条件概率，对比普通的采样是直接根据有多少个才看作是w，这个是每个w不同</p>
<p><img src="/2022/10/29/AI复习总结/37.png" alt=""></p>
<p>还有一种给你随机序列让你完成一组采样和weight计算的类型</p>
<p>estimate$P(C=1|B=1,E=1)$，已知B,E为evidence，要采样剩余的A,C,D</p>
<p>采样标准：select a value <img src="http://latex.codecogs.com/svg.latex?a" alt=""> from the table, and choose $W=1$if$a \geq P(W=0) $</p>
<p>根据第一个值0.249&gt;0.2,A=1,0.052&lt;0.6,C=0,0,299&lt;0.6 D=0</p>
<p>故A=1,C=0,D=0</p>
<p>$weight=P(B|A)P(\neg C,\neg D|E)=0.8*0.8=0.64$<br><img src="/2022/10/29/AI复习总结/40.png" alt=""></p>
<p><img src="/2022/10/29/AI复习总结/39.png" alt=""></p>
<h2 id="Gibbs-Sampling-吉布斯采样"><a href="#Gibbs-Sampling-吉布斯采样" class="headerlink" title="Gibbs Sampling(吉布斯采样)"></a>Gibbs Sampling(吉布斯采样)</h2><p>先确定一个evidence，已经有一个固定值了，然后随机初始化其他的所有变量，之后随便选一个不是evidence I的变量进行采样，现要求这个I的马尔可夫毯的条件概率如下图就是</p>
<p>先求weight$w(s|c,r,\neg w)=P(s|c)P(\neg w|s,r),w(\neg s|c,r,\neg w)=P(\neg s|c)P(\neg w|\neg s,r)$</p>
<p>再求$p(s|c,r,\neg w)=w(s|c,r,\neg w)/(w(s|c,r,\neg w)+w(\neg s|c,r,\neg w))$</p>
<p>$p(\neg s|c,r,\neg w)=w(\neg s|c,r,\neg w)/(w(s|c,r,\neg w)+w(\neg s|c,r,\neg w))$</p>
<p>一般来说是直接算完就行了，可能会给一个接受率然后根据这个来决定sample T还是F，可能会给一个随机值r让$r \leq p(s|c,r,\neg w))$就取T，不然就是F~~~~</p>
<p><img src="/2022/10/29/AI复习总结/38.png" alt=""></p>
<h2 id="MCMC-Markov-Chain-Monte-Carlo"><a href="#MCMC-Markov-Chain-Monte-Carlo" class="headerlink" title="MCMC(Markov Chain Monte Carlo)"></a>MCMC(Markov Chain Monte Carlo)</h2><p>Markov chain = a sequence of randomly chosen states (“random walk”), where each state is chosen conditioned on the previous state</p>
<p>MCMC = sampling by constructing a Markov chain</p>
<p>基本思路：在随机变量x的状态空间S上定义一个满足遍历定理的马尔可夫链$X=x_0,x_1,…,x_t,….$，使得平稳分布就是抽样的目标分布$p(x)p ( x ) 。然后在这个马尔可夫链上开始随机游走，每个时刻得到一个样本。根据遍历定理，当时间趋于无穷时，样本的分布是趋于平稳分布的，样本的函数均值趋于函数的期望均值。所以说当时间足够长时(比如说当n&gt;m时)(从时刻1到m我们称之为燃烧期)，在m时刻之后的时间里随机游走得到的样本值就是对目标分布抽样的结果，得到的函数均值就是近似函数的数学期望。</p>
<p>马尔可夫蒙特卡罗法基本步骤：</p>
<ul>
<li><p>首先，在随机变量x的状态空间S上构造一个满足遍历定理的马尔科夫链，使其平稳分布为目标分布$p(x)$。</p>
</li>
<li><p>从状态空间中的某一点开始出发，用构造的马尔科夫链进行随机游走，产生样本序列$x_0,x_1,…,x_t$。</p>
</li>
<li><p>确定正整数m和n，得到样本值集合$x_{m+1},…,x_n$。求的函数$f(x)$的均值：$E_{p(x)}[f(x)]=\frac{1}{n-m}\sum_{i=m+1}^nf(x_i)$</p>
</li>
</ul>
<h3 id="Metropolis-Hastings"><a href="#Metropolis-Hastings" class="headerlink" title="Metropolis-Hastings"></a>Metropolis-Hastings</h3><p>首先先定义采样时刻t-1的采样值为x，t时刻的采样值为x’，所以对于要抽样的概率分布$p(x)$采用转移核为$p(x,x’)$的马尔科夫链：$p(x,x’)=q(x,x’)\alpha(x,x’)$</p>
<p>其中$q(x,x’)$和$\alpha(x,x’)$分布被称之为提议分布和接受率。而提议分布是另一个马尔可夫链的转移核，是一个比较容易抽样的分布。而接受分布$\alpha(x,x’)$是：$\alpha(x,x’)=min(1,\frac{p(x’)q(x’,x)}{p(x)q(x,x’)})$</p>
<ul>
<li><p>任意选一个初始值$x_0$</p>
</li>
<li><p>对于$i=1,2,…,n$ 循环操作 ①对于$x_{i-1}=x$按分布$q(x,x’)$ 随机抽取一个候选状态$x’$②计算$\alpha(x,x’)$③在(0,1)中去一个随机值如果$\alpha(x,x’)\geq u$<br>则$x_i=x’$否则$x_i=x$</p>
</li>
<li><p>最后得到样本合集$x_{m+1},…,x_n$，算$E_{p(x)}[f(x)]=\frac{1}{n-m}\sum_{i=m+1}^nf(x_i)$</p>
</li>
</ul>
<p>吉布斯采样就是接受值一直为1的特例，一直接受新采样的样本</p>
<h1 id="Markov-logic"><a href="#Markov-logic" class="headerlink" title="Markov logic"></a>Markov logic</h1><p> A Markov Logic Network (MLN) is <strong>a set of pairs (F, w)</strong></p>
<p>where</p>
<ul>
<li><p>F is a formula in first-order logic</p>
</li>
<li><p>w is a real number</p>
</li>
</ul>
<p>Together with a set of <strong>constants</strong>, it defines a Markov network with</p>
<ul>
<li><p>One node for each grounding of each predicate in the MLN</p>
</li>
<li><p>One clique for each grounding of each formula F in the MLN, with the potential being: exp(w) for node assignments that satisfy F, 1 otherwise</p>
</li>
</ul>
<p><img src="/2022/10/29/AI复习总结/61.png" alt=""></p>
<p><img src="/2022/10/29/AI复习总结/62.png" alt=""></p>
<p><strong>MLN is template for ground Markov nets</strong></p>
<p>Probability of a word x: $P(x)=\frac{1}{Z}(\sum_iw_in_i(x))$,$w_i$ weight of formula i, $n_i(x)$ No. of true groundings of formula i in x</p>
<p><img src="/2022/10/29/AI复习总结/63.png" alt=""></p>
<p>Infinite weights -&gt; First-order logic</p>
<p><strong>Markov logic allows contradictions between formulas</strong></p>
<h3 id="Relation-to-PRM"><a href="#Relation-to-PRM" class="headerlink" title="Relation to PRM"></a>Relation to PRM</h3><ul>
<li><p>MLN is More general and flexible than PRM</p>
</li>
<li><p>In principle, a PRM can be converted into a MLN by writing a formula for each entry of each CPT and setting the weight to be the logarithm of the conditional probability</p>
</li>
</ul>
<p><img src="/2022/10/29/AI复习总结/64.png" alt=""></p>
<h1 id="Markov-Models"><a href="#Markov-Models" class="headerlink" title="Markov Models"></a>Markov Models</h1><p>▪  <strong>Past and future independent given the present</strong></p>
<p>▪ <strong>Each time step only depends on the previous</strong></p>
<p>first-order Markov model</p>
<h3 id="Stationary-distribution"><a href="#Stationary-distribution" class="headerlink" title="Stationary distribution"></a>Stationary distribution</h3><p>求无限的分布时候：$P_{\infty }(X)=\sum_xP(X|x)P_{\infty}(x) $</p>
<p><img src="/2022/10/29/AI复习总结/41.png" alt=""></p>
<h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><p>If the hidden state at time t is given, both the evidence at time t and the hidden state at time t + 1 are independent of the hidden state at time t − 1.</p>
<ol>
<li><p>先求$P(x_t|e_{1:t})$，一般为已知，没有e0就直接用，有先算$P(x_0|e_0)=P(e_0|x_0)P(x_0)$</p>
</li>
<li><p>再求$P(e_{t+1}|x_{t+1})\sum_{x_t}P(x_t|e_{1:t})P(X_{t+1}|x_t)$</p>
</li>
<li><p>最后标准化</p>
</li>
</ol>
<p>$P(X_{t+1}|e_{1:t})=\sum_{X_t}P(X_t|e_{1:t})P(X_{t+1}|X_t,e_{1:t})$</p>
<p><img src="/2022/10/29/AI复习总结/42.png" alt=""></p>
<h3 id="Forward-algorithm"><a href="#Forward-algorithm" class="headerlink" title="Forward algorithm"></a>Forward algorithm</h3><p>cost per time:$O(|X|^2)$,$|X|$ 是state数量</p>
<p>$f_{1:t+1}=\alpha P(e_{t+1}|x_{t+1})\sum_{x_t}f_{1:t}[x_t]P(X_{t+1}|x_t)$</p>
<p><img src="/2022/10/29/AI复习总结/43.png" alt=""></p>
<h3 id="Viterbi-algorithm"><a href="#Viterbi-algorithm" class="headerlink" title="Viterbi algorithm"></a>Viterbi algorithm</h3><p>Forward求和，这个求最大值</p>
<p>$m_{1:t+1}=P(e_{t+1}|x_{t+1})max_{x_t}{m_{1:t}[x_t]P(X_{t+1}|x_t)}$</p>
<p>Time:$O(|X|^2T)$, Space $O(|X|T)$,T length of sequence</p>
<p>Both the time complexity and the space complexity of the Viterbi algorithm are linear in  the length of the sequence.</p>
<p><img src="/2022/10/29/AI复习总结/45.png" alt=""></p>
<p>两者比较，时间复杂度相同</p>
<p><img src="/2022/10/29/AI复习总结/44.png" alt=""></p>
<h2 id="Dynamic-Bayes-Nets"><a href="#Dynamic-Bayes-Nets" class="headerlink" title="Dynamic Bayes Nets"></a>Dynamic Bayes Nets</h2><p>▪ Every HMM is a DBN，hmm都是dbn</p>
<ul>
<li>Every hidden Markov model can be represented as a dynamic Bayesian network with a single state variable and a single evidence variable.</li>
</ul>
<p>▪ Each HMM state is Cartesian product of DBN state variables</p>
<p>▪ Every discrete DBN can be represented by a HMM，只有离散的dbn才是hmm</p>
<p>DBN优点：参数更少，Sparse dependencies</p>
<h2 id="Particle-Filtering"><a href="#Particle-Filtering" class="headerlink" title="Particle Filtering"></a>Particle Filtering</h2><p>Represent belief state at each step by a set of samples（Samples are called particles）</p>
<p>Each iteration of particle fifiltering consists of propagate forward, observe and resample</p>
<p><strong>Initialization</strong></p>
<p>▪ sample N particles from the initial distribution $P(X_0 ）$，All particles have a weight of 1</p>
<p>▪ Each particle is moved by sampling its next position from the transition model:$x_{t+1} $~$P(X_{t+1}|x_t)$</p>
<p>▪ Similar to likelihood weighting, weight samples based on the evidence $W=P(e_t|x_t)$</p>
<p>▪ Rather than tracking weighted samples, we resample. Each new sample is selected from the current population of samples; the probability is proportional to its weight</p>
<p><img src="/2022/10/29/AI复习总结/46.png" alt=""></p>
<h1 id="Markov-Decision-Processes"><a href="#Markov-Decision-Processes" class="headerlink" title="Markov Decision Processes"></a>Markov Decision Processes</h1><p><strong>MDPs are non-deterministic search problems</strong></p>
<p>MDP satisfy the memoryless property.</p>
<p>马尔可夫决策过程是一个五元组，求解一个马尔可夫决策过程，则意味着找到最优<strong>策略π*</strong></p>
<p>S：有限状态集</p>
<p>A：有限动作集</p>
<p>P：状态转移矩阵 (也有的地方是T)</p>
<p>R：奖励函数</p>
<p>γ：折扣因子 是给定状态的动作分布 <strong>状态值函数：</strong><br> <strong>动作值函数：</strong>  状态值函数的<strong>贝尔曼期望方程</strong>：某一个状态的价值可以用该状态下所有动作的价值表述  动作值函数的<strong>贝尔曼期望方程</strong>： 某一个动作的价值可以用该状态后继状态的价值表述，及 发生了动作a的价值</p>
<h3 id="贝尔曼-Bellman-方程"><a href="#贝尔曼-Bellman-方程" class="headerlink" title="贝尔曼(Bellman)方程"></a><strong>贝尔曼(Bellman)方程</strong></h3><ol>
<li>一个状态s的最优值$V^*(s)$ </li>
</ol>
<p>——s的最优值是一个从s出发的agent在其余下寿命中采取最优行动能获得的效益的期望值。</p>
<ol start="2">
<li>一个q状态(s,a)的最优值$Q^*(s,a)$ </li>
</ol>
<p>——(s,a)的最优值是一个agent从状态s采取行动a之后获得的效益的期望值，并且该agent从此以后采取的都是最优行动。</p>
<ol start="3">
<li>最优策略$\pi ^*(s)$</li>
</ol>
<p>$Q^<em>(s,a)=\sum_{s’}T(s,a,s’)[R(s,a,s’)+ \gamma V^</em>(s’)]$,$V^<em>(s)=max_a\sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma V^</em>(s’)]$</p>
<h3 id="值迭代-Value-Iteration-求解马尔科夫决策过程"><a href="#值迭代-Value-Iteration-求解马尔科夫决策过程" class="headerlink" title="值迭代 (Value Iteration)求解马尔科夫决策过程"></a><strong>值迭代 (Value Iteration)求解马尔科夫决策过程</strong></h3><p>现在我们有了一个能验证MDP中各状态的值的最优性的框架，接下来自然就想知道如何能精确计算出这些最优值。为此我们需要<strong>限时值（time-limited values）</strong>（强化有限界得到的结果）。限制时间步数为k的一个状态s的限时值表示为 $V_k(s)$ ，代表着在已知当前MDP会在k时间步后终止的情况下，从s出发能得到的最大期望效益。这也正是一个在MDP的搜索树上执行的k层expectimax所返回的东西。</p>
<blockquote>
<p>简单来说，值迭代就是给定k，也就是步数为k，在这k步之内，我们希望值迭代已经到达稳定状态，即下一次迭代与这一次的完全相同，这时我们就得到一个最优决策，也就是从哪里出发能够得到最大期望效益。</p>
</blockquote>
<p>$V_{k+1}(s)\leftarrow max_a \sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma V_k(s’)]$</p>
<p>Time:$O(S^2A)$</p>
<p>$V_k$ converge as long as $\gamma &lt;1$</p>
<h3 id="策略迭代-Policy-Iteration求解马尔科夫决策过程"><a href="#策略迭代-Policy-Iteration求解马尔科夫决策过程" class="headerlink" title="策略迭代 Policy Iteration求解马尔科夫决策过程"></a><strong>策略迭代 Policy Iteration求解马尔科夫决策过程</strong></h3><p>Time:$O(S^2A)$ </p>
<p><strong>策略提取 Policy Extraction</strong></p>
<p>还记得我们解决MDP的最终目标是要确定一个最优策略。只需要确定所有状态的最优值就能达成这一目标，这可以通过一种叫策略提取(policy extraction)的方法来实现。策略提取的背后的思想非常简单：如果你处于一种状态s，你应该采取会产生最大期望效益的行动a。不难想到，a正是会将我们带到具有最大q值的q状态的操作，于是最优策略可以表达为：</p>
<p>$\pi^<em>(s)=argmax_a\sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma V^</em>(s’)]$</p>
<p>为了取得最好的效益，状态的最优q值对策略提取来说是最好的，因为在这种情况下，只需要一次argmax就能确定从一个状态出发的最优行动。仅保存每个 $V^*(s)$意味着我们必须在取argmax之前用Bellman方程重新计算所有必须的q值，相当于进行一次深度为1的expectimax。</p>
<p><img src="/2022/10/29/AI复习总结/47.png" alt=""></p>
<p><strong>策略迭代 Policy Iteration</strong></p>
<p>值迭代可能会很慢。在每次迭代中，我们必须更新所有|S|个状态的值（|n|表示集合中元素的个数），其中每个都要求在我们计算每个行动的q值时对所有|A|个行动进行迭代。对这些每个q值的计算，需要轮流对|S|个状态再次进行迭代，导致时间成本过高  。此外，当我们只想确定MDP的最优策略时，值迭代会进行大量多余的计算，因为由策略提取得到的策略通常会比值本身更快地收敛。修正这些缺陷的方法就是选择策略迭代，这种算法可以在保持值迭代的最优性的同时还能对表现进行大幅提升。策略迭代的操作如下：</p>
<ol>
<li><p>定义一个初始策略。这个可以随意定，但是如果初始策略越接近最优策略，策略迭代收敛得也就越快。</p>
</li>
<li><p>重复以下操作，直至收敛：  </p>
</li>
<li><p>用<strong>策略评估</strong>对当前策略进行评估。对于一个策略π，策略评估意味着计算所有状态s的Vπ(s)，其中Vπ(s)表示按照策略π从状态s出发的期望效益：   $V^\pi (s)=max_a\sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma V^\pi(s’)]$</p>
</li>
<li><p>把策略迭代的第i次迭代成为πi。由于我们正在对每个状态的一个行动进行修正，我们不再需要取最大的操作max operator，这样我们得到的系统就能有效由以上规则生成的|S|个方程构成的。然后每个$V^{\pi_i}(s)$ 就可以通过解决这个系统来计算得到。</p>
</li>
</ol>
<p>评估完当前策略后，用<strong>策略改进 policy improvement</strong>来生成更好的策略。策略改进通过对由策略评估生成的状态的值进行策略提取，生成以下改进提升后的策略：</p>
<p>$\pi_{i+1}(s)=argmax_a\sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma V^{\pi_i}(s’)]$</p>
<p>策略迭代就是给定初始策略，不断进行策略评估-&gt;策略提取-&gt;策略改进直至收敛</p>
<h3 id="值迭代和策略迭代比较"><a href="#值迭代和策略迭代比较" class="headerlink" title="值迭代和策略迭代比较"></a>值迭代和策略迭代比较</h3><p>▪ Both value iteration and policy iteration compute the same thing (all optimal values)计算一样的东西</p>
<p>▪<strong>In value iteration:</strong></p>
<p>▪ Every iteration updates both the values and (implicitly) the policy</p>
<p>▪ We don’t track the policy, but taking the max over actions implicitly recomputes it</p>
<p>▪ <strong>In policy iteration:</strong></p>
<p>▪ We do several passes that update utilities with fixed policy (each pass is fast because we consider only one action, not all of them)</p>
<p>▪ After the policy is evaluated, a new policy is chosen (slow like a value iteration pass)</p>
<p>▪ May converge faster，会更快收敛</p>
<p><strong>Both are dynamic programs for solving MDPs</strong></p>
<h1 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h1><p>Offline Planning</p>
<p>▪ You determine all quantities through computation</p>
<p>▪ You need to know the details of the MDP</p>
<p>▪ You do not actually play the game!</p>
<p>Online Planning</p>
<p>不需要transition model for states and actions，reward funcition for every allowed transition between states</p>
<p><img src="/2022/10/29/AI复习总结/48.png" alt=""></p>
<h2 id="Model-Based-Learning"><a href="#Model-Based-Learning" class="headerlink" title="Model-Based Learning"></a>Model-Based Learning</h2><p><strong>离线学习</strong>(off-line)也通常称为批学习，是指对独立数据进行训练，将训练所得的模型用于预测任务中。将全部数据放入模型中进行计算，一旦出现需要变更的部分，只能通过再训练(retraining)的方式，这将花费更长的时间，并且将数据全部存在服务器或者终端上非常占地方，对内存要求高。Q学习就是离线学习</p>
<p><strong>在线学习</strong>(in-line)也称为增量学习或适应性学习，是指对一定顺序下接收数据，每接收一个数据，模型会对它进行预测并对当前模型进行更新，然后处理下一个数据。这对模型的选择是一个完全不同，更复杂的问题。需要混合假设更新和对每轮新到达示例的假设评估。换句话说，你只能访问之前的数据，来回答当前的问题。</p>
<h2 id="Model-Free-Learning"><a href="#Model-Free-Learning" class="headerlink" title="Model-Free Learning"></a>Model-Free Learning</h2><ul>
<li><p>Input: a fixed policy (s)</p>
</li>
<li><p>You don’t know the transitions T(s,a,s’)</p>
</li>
<li><p>You don’t know the rewards R(s,a,s’)</p>
</li>
<li><p>Goal: learn the state values</p>
</li>
</ul>
<h3 id="Direct-Evaluation"><a href="#Direct-Evaluation" class="headerlink" title="Direct Evaluation"></a>Direct Evaluation</h3><p>Goal: Compute values for each state under $\pi$</p>
<p>Idea: Average together observed sample values</p>
<p>▪ Act according to $\pi$</p>
<p>▪ Every time you visit a state, write down what the sum of discounted rewards turned out to be</p>
<p>▪ Average those samples</p>
<p><strong>Good</strong>: </p>
<p>▪ It’s easy to understand</p>
<p>▪ It doesn’t require any knowledge of T, R</p>
<p>▪ It eventually computes the correct average values, using just sample transitions</p>
<p><strong>Bad</strong>:<br>▪ It wastes information about state connections</p>
<p>▪ Each state must be learned separately</p>
<p>▪ So, it takes a long time to learn</p>
<h3 id="Passive-Reinforcement-Learning"><a href="#Passive-Reinforcement-Learning" class="headerlink" title="Passive Reinforcement Learning"></a>Passive Reinforcement Learning</h3><p>TD学习的基本思想是<strong>从每一次经验中学习</strong>（这里我的理解是从v中学习，一定程度上也可以称为v学习），而不是像直接评估那样简单地记录总奖励和访问状态并在最后学习。在策略评估中，我们使用固定策略和贝尔曼方程产生的方程评估该策略下的状态值：</p>
<p><strong>Temporal Difference Learning</strong></p>
<p>▪ (Policy still fixed, still doing evaluation!)</p>
<p>▪ Move the value towards the sample</p>
<p><img src="/2022/10/29/AI复习总结/49.png" alt=""></p>
<p>Decreasing learning rate (alpha) can give converging averages</p>
<h2 id="Model-Based-Learning-1"><a href="#Model-Based-Learning-1" class="headerlink" title="Model-Based Learning"></a>Model-Based Learning</h2><p>▪ Learn an approximate model based on experiences</p>
<p>▪ Solve for values as if the learned model was correct</p>
<ul>
<li><p>Step 1: Learn empirical MDP model</p>
</li>
<li><p>Step 2: Solve the learned MDP</p>
</li>
</ul>
<h3 id="Active-Reinforcement-Learning"><a href="#Active-Reinforcement-Learning" class="headerlink" title="Active Reinforcement Learning"></a>Active Reinforcement Learning</h3><p><img src="/2022/10/29/AI复习总结/50.png" alt=""></p>
<p>Q-learning can converge to the optimal policy even if you are acting sub-optimally when you explore.</p>
<p>The factor for Q-learning to converge to the optimal Q-values:</p>
<ul>
<li><p>Every state-action pair is visited infinitely often.</p>
</li>
<li><p>The learning rate is decreased to 0 over time.</p>
</li>
</ul>
<p><strong>Full reinforcement learning</strong></p>
<p>▪ You don’t know the transitions T(s,a,s’)</p>
<p>▪ You don’t know the rewards R(s,a,s’)</p>
<p>▪ You choose the actions now</p>
<p>▪ Goal: learn the optimal policy / values</p>
<p>Q-learning：off-policy learning</p>
<p><img src="/2022/10/29/AI复习总结/51.png" alt=""></p>
<h3 id="Regret"><a href="#Regret" class="headerlink" title="Regret"></a>Regret</h3><p>The $\epsilon $-greedy strategy often produces a lower regret for a decaying $\epsilon$ than a fixed one.</p>
<p>Regret is a measure of your total mistake cost: the difference between your (expected) rewards, including youthful suboptimality, and optimal (expected) rewards</p>
<p>Minimizing regret goes beyond learning to be optimal – it requires optimally learning to be optimal</p>
<h2 id="Approximate-Q-Learning"><a href="#Approximate-Q-Learning" class="headerlink" title="Approximate Q-Learning"></a>Approximate Q-Learning</h2><p>Approximate Q-learning is model-free learning</p>
<p>▪ Learn about some small number of training states from experience</p>
<p>▪ Generalize that experience to new, similar situations</p>
<p>Compared with Q-learning, approximate Q-learning that describes each state with a vector of features can be helpful when the number of states is huge.</p>
<h3 id="Linear-Value-Functions"><a href="#Linear-Value-Functions" class="headerlink" title="Linear Value Functions"></a>Linear Value Functions</h3><p><img src="/2022/10/29/AI复习总结/52.png" alt=""></p>
<p>  Q-learning:</p>
<p>transition=$(s,a,r,s’)$</p>
<p>difference=$[r+\gamma max_{a’}Q(s’,a’)]-Q(s,a)$</p>
<p>$Q(s,a)=Q(s,a)+\alpha [difference],w_i=w_i+\alpha [difference]f_i(s,a)$</p>
<h1 id="Supervised-Machine-Learning"><a href="#Supervised-Machine-Learning" class="headerlink" title="Supervised Machine Learning"></a>Supervised Machine Learning</h1><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naïve Bayes"></a>Naïve Bayes</h2><p>$P(Y,F_1,…,F_n)=P(Y)\prod _iP(F_i|Y)$</p>
<p>nx|F|x|Y| parameters</p>
<p><strong>Total number of parameters is linear in n</strong></p>
<p>Goal: compute posterior distribution over label variable</p>
<p>The na ̈ıve Bayes assumption takes all features to be independent given the class label.</p>
<h2 id="Overfitting过拟合"><a href="#Overfitting过拟合" class="headerlink" title="Overfitting过拟合"></a>Overfitting过拟合</h2><p><strong>原因：</strong></p>
<p>1.训练集的数据太少</p>
<p>2.训练集和新数据的特征分布不一致</p>
<p>3.训练集中存在噪音。噪音大到模型过分记住了噪音的特征，反而忽略了真实的输入输出间的关系。</p>
<p>4.权值学习迭代次数足够多，拟合了训练数据中的噪音和训练样例中没有代表性的特征。</p>
<p><strong>解决方法：</strong></p>
<p>加大数据集（Training and test Set）使模型在训练的过程中尽可能多的见到更多的问题，从而使学到的分布更加全面更加General。比如采集更多的样本，或者采取Data Augmentation的方法。</p>
<p>改进你的模型。<br>2.1. 比如将你的模型进行简化，从而减少一些不重要的参数和属性。比如如果你使用神经网络，你可以减少Hidden Layer，或者减少每个Hidden Layer中的神经元数量（Dropout等）；或者如果你使用Adaboost这类的Ensemble Learning方法，你可以减少模型所包含的Weak Learner的数量，并且尽可能的简化每一个Weak Learner。<br>2.2 你也可以采取一些Regularization的方法主动地调节模型的参数值，比如L1， L2 Norm等等。采用这类方法，可以很大程度的惩罚模型内部的参数，从而阻止Overfitting的发生。（之后的专栏会有介绍）。</p>
<p>提前结束无意义的训练 Early Stop。比如当我们训练一个Logistic Regression时，我们的目标是专注于找到合适的参数，使Loss（error）尽可能的小。如果重复过多次数的训练，会导致训练出来的模型过分专注于解决训练集，从而使模型不具备Generalization。如下图，可以看到，过多的training iteration，确实可以降低Training Loss，但是也会使模型过分专注于training Samples,从而使Test Loss提高（即Overfit）。这时候我们需要提前结束训练，阻止Test Error的提升。</p>
<p><strong>Avoid overfitting</strong></p>
<ul>
<li><p>Acquire more training data (not always possible)</p>
</li>
<li><p>Remove irrelevant attributes (not always possible)</p>
</li>
<li><p>Limit the model expressiveness by regularization, early stopping, pruning, etc.</p>
</li>
<li><p>We can use smoothing or regularization to improve model’s generalization.</p>
</li>
</ul>
<p>we may smooth the empirical rate to improve generalization</p>
<h2 id="Laplace-Smoothing"><a href="#Laplace-Smoothing" class="headerlink" title="Laplace Smoothing"></a>Laplace Smoothing</h2><p><img src="/2022/10/29/AI复习总结/53.png" alt=""></p>
<p>$c(x)$权重，k给定，|X|x有多少取值，N x权重和</p>
<h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>Least squares: Minimizing squared error最小二乘法</p>
<h1 id="Unsupervised-Machine-Learning"><a href="#Unsupervised-Machine-Learning" class="headerlink" title="Unsupervised Machine Learning"></a>Unsupervised Machine Learning</h1><h2 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h2><p><img src="/2022/10/29/AI复习总结/54.png" alt=""></p>
<h2 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h2><p>The EM algorithm can be used to learn any model with hidden variables (missing data)</p>
<p>[E step] Compute probability of each instance having each possible label，Compute label distribution of each data point</p>
<p>[M step] Treating each instance as fractionally having both labels,</p>
<p>compute the new parameter values，Compute weighted MLE of parameters given label distributions, When using the EM algorithm to learn a Gaussian mixture model, the M-step computes weighted maximum likelihood estimation (MLE).</p>
<p><img src="/2022/10/29/AI复习总结/55.png" alt=""></p>
<p>The label distributions computed at E-step are point-estimations</p>
<p><img src="/2022/10/29/AI复习总结/56.png" alt=""></p>
<h3 id="EM-amp-K-means"><a href="#EM-amp-K-means" class="headerlink" title="EM &amp; K-means"></a>EM &amp; K-means</h3><p><img src="/2022/10/29/AI复习总结/65.png" alt=""></p>
<p>A neural network with a sufficient number of neurons and a complex enough architecture can approximate any continuous function to any desired accuracy.</p>
<p>In Convolutional Neural Networks, different hidden units organized into the same “feature map” share weight parameters.</p>
<h1 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h1><h2 id="Context-Free-Grammars"><a href="#Context-Free-Grammars" class="headerlink" title="Context-Free Grammars"></a>Context-Free Grammars</h2><p><strong>four components</strong></p>
<p> A set of terminals (words)</p>
<p> A set N of nonterminals (phrases)</p>
<p> A start symbol SN</p>
<p> A set R of production rules</p>
<p> Specifies how a nonterminal can produce a string of terminals and/or nonterminals</p>
<p>Any arbitrary CFG can be rewritten into the Chomsky normal form (CNF), Probabilistic CFGs tend to assign larger probabilities to smaller parse trees.</p>
<h2 id="CFG-Chomsky-Normal-Form-，乔姆斯基规范形式"><a href="#CFG-Chomsky-Normal-Form-，乔姆斯基规范形式" class="headerlink" title="CFG(Chomsky Normal Form)，乔姆斯基规范形式"></a>CFG(Chomsky Normal Form)，乔姆斯基规范形式</h2><p>具有以下形式：</p>
<pre class="line-numbers language-rust"><code class="language-rust">A <span class="token punctuation">-></span> a
A <span class="token punctuation">-></span> BC
S <span class="token punctuation">-></span> ε
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>小写字母表示终结符号，大写表示非终结符。<br>产生式的体要么由单个终结符号或者ε构成，要么由两个非终结符构成。</p>
<p>题目转换只需要满足箭头后面只有两个大写单词，不会是一个或者三个，多于两个用x新增一行，只有一个向下面合并</p>
<p><img src="/2022/10/29/AI复习总结/57.png" alt=""></p>
<p>Ambiguity: A sentence is ambiguous if it has more than one possible parse tree</p>
<p>Probabilistic CFGs have ambiguity problem.</p>
<h2 id="Cocke–Younger–Kasami-Algorithm-CYK"><a href="#Cocke–Younger–Kasami-Algorithm-CYK" class="headerlink" title="Cocke–Younger–Kasami Algorithm (CYK)"></a>Cocke–Younger–Kasami Algorithm (CYK)</h2><p>Base case:</p>
<p> A is in cell [i-1,i] iff. there exists a rule A → wi</p>
<p>Recursion:</p>
<p> A is in cell [i,j] iff. for some rule A → B C there is a B in ell [i,k] and a C in cell [k,j] for some k.</p>
<p><img src="/2022/10/29/AI复习总结/58.png" alt=""><br>图示</p>
<p><img src="/2022/10/29/AI复习总结/59.png" alt=""></p>
<h2 id="Dependency-Parse"><a href="#Dependency-Parse" class="headerlink" title="Dependency Parse"></a>Dependency Parse</h2><p><img src="/2022/10/29/AI复习总结/60.png" alt=""></p>
<p>The links between the words represent their dependency relations </p>
<p>Dependency parses of sentences having the same meaning are more similar across languages than constituency parses</p>
<p>Parsing can be faster than CFG-bases parsers</p>
<h2 id="Graph-based-parsing"><a href="#Graph-based-parsing" class="headerlink" title="Graph-based parsing"></a>Graph-based parsing</h2><p>Parsing: find the highest-scoring parse tree</p>
<p>Taking a string and a grammar and returning one or more parse tree(s) for that string</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 valine -->
<div id="comment">
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#comment' ,
        notify: false,
        verify: false,
        app_id: 'fUtpOauFo2JhcWoBFEnnppJW-gzGzoHsz',
        app_key: 'gFTwG42ACHuyrmwhMfgxcRQQ',
        placeholder: '留下你的评论吧~~',
        pageSize: '10',
        avatar: '',
        avatar_cdn: 'https://gravatar.loli.net/avatar/'
    });
</script>
</div>
<style>
   #comment{
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
            	<span id="busuanzi_container_site_pv">2023总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"left","width":125,"height":250},"mobile":{"show":false},"log":false});</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"left","width":125,"height":250},"mobile":{"show":false},"log":false});</script></body>




 	

    </script>
</html>
