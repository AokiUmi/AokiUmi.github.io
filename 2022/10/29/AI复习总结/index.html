<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="img/love.jpg"/>
	<link rel="shortcut icon" href="img/love.jpg">
	
			    <title>
    Aoki_Umi
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
 <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <span id="busuanzi_container_site_pv">��վ�ܷ�����<span id="busuanzi_value_site_pv"></span>��</span>
    <meta name="keywords" content="Aoki_Umi" />
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<link rel="alternate" href="atom.xml" title="Aoki_Umi" type="application/atom+xml">
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<script>hljs.initHighlightingOnLoad();</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">AOKI_UMI</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">HOME</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">CATEGORIES</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="../categories/CS/">CS</a></li><li><a class="category-link" href="../categories/CS/AI/">AI</a></li><li><a class="category-link" href="../categories/CS/ALGORITHM/">ALGORITHM</a></li><li><a class="category-link" href="../categories/CS/ASSEMBLY-LANGUAGE/">ASSEMBLY_LANGUAGE</a></li><li><a class="category-link" href="../categories/CS/C-C/">C&C++</a></li><li><a class="category-link" href="../categories/CS/CA/">CA</a></li><li><a class="category-link" href="../categories/CS/PYTHON/">PYTHON</a></li><li><a class="category-link" href="../categories/CS/SIGNAL-PROCESSING/">SIGNAL_PROCESSING</a></li><li><a class="category-link" href="../categories/OI/">OI</a></li><li><a class="category-link" href="../categories/OI/ALGORITHM/">ALGORITHM</a></li><li><a class="category-link" href="../categories/OI/Algorithm/">Algorithm</a></li><li><a class="category-link" href="../categories/OI/Algorithm/Tree/">Tree</a></li><li><a class="category-link" href="../categories/OI/Character-String/">Character String</a></li><li><a class="category-link" href="../categories/OI/Congratulation/">Congratulation</a></li><li><a class="category-link" href="../categories/OI/Contest/">Contest</a></li><li><a class="category-link" href="../categories/OI/DATA-STRUCTURE/">DATA STRUCTURE</a></li><li><a class="category-link" href="../categories/OI/DP/">DP</a></li><li><a class="category-link" href="../categories/OI/GRAPH-THERY/">GRAPH THERY</a></li><li><a class="category-link" href="../categories/OI/Graph-Theory/">Graph Theory</a></li><li><a class="category-link" href="../categories/OI/Maths/">Maths</a></li><li><a class="category-link" href="../categories/OI/Search/">Search</a></li><li><a class="category-link" href="../categories/OI/Simulation/">Simulation</a></li><li><a class="category-link" href="../categories/STUDY/">STUDY</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">ARCHIVES</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="../archives/2022/10/">October 2022</a></li><li><a class="archive-link" href="../archives/2022/06/">June 2022</a></li><li><a class="archive-link" href="../archives/2022/01/">January 2022</a></li><li><a class="archive-link" href="../archives/2021/11/">November 2021</a></li><li><a class="archive-link" href="../archives/2021/04/">April 2021</a></li><li><a class="archive-link" href="../archives/2021/01/">January 2021</a></li><li><a class="archive-link" href="../archives/2020/11/">November 2020</a></li><li><a class="archive-link" href="../archives/2020/10/">October 2020</a></li><li><a class="archive-link" href="../archives/2020/08/">August 2020</a></li><li><a class="archive-link" href="../archives/2020/06/">June 2020</a></li><li><a class="archive-link" href="../archives/2020/05/">May 2020</a></li><li><a class="archive-link" href="../archives/2020/04/">April 2020</a></li><li><a class="archive-link" href="../archives/2020/02/">February 2020</a></li><li><a class="archive-link" href="../archives/2018/11/">November 2018</a></li><li><a class="archive-link" href="../archives/2018/10/">October 2018</a></li><li><a class="archive-link" href="../archives/2018/09/">September 2018</a></li><li><a class="archive-link" href="../archives/2018/08/">August 2018</a></li><li><a class="archive-link" href="../archives/2018/07/">July 2018</a></li><li><a class="archive-link" href="../archives/2018/05/">May 2018</a></li><li><a class="archive-link" href="../archives/2018/04/">April 2018</a></li><li><a class="archive-link" href="../archives/2018/03/">March 2018</a></li><li><a class="archive-link" href="../archives/2018/01/">January 2018</a></li><li><a class="archive-link" href="../archives/1924/07/">July 1924</a>
	                    </ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="ABOUT">
		                ABOUT
		            </a>
		        </li>
		        
		        <li>
		            <a href="/group/" title="FRIENDS">
		                FRIENDS
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="GALLERY">
		                GALLERY
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="TAG">
		                TAG
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/AokiUmi" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="500px" href="http://500px.com" target="_blank" rel="noopener">
                            <i class="icon fa fa-500px"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(/images/0037.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >AI复习总结</h2></a>
         
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">

                <p>参考有很多，不一一列举，还有很多懒得找了（</p>
<p><a href="https://blog.csdn.net/weixin_43407273/article/details/110856123" target="_blank" rel="noopener">MCMC采样详解_、寄生于黑暗中的光，的博客-CSDN博客_mcmc采样</a></p>
<p><a href="https://www.cnblogs.com/wt869054461/p/9899929.html" target="_blank" rel="noopener">马尔可夫毯（Markov Blanket） - 夕月一弯 - 博客园</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/436214290" target="_blank" rel="noopener">D-separation： 判断贝叶斯网络中的变量是否独立</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/102014899" target="_blank" rel="noopener"> 概率图模型(PGMs)-马尔可夫网络(Markov Nets)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/521329634" target="_blank" rel="noopener">人工智能导论考前复习笔记</a></p>
<h1 id="Search"><a href="#Search" class="headerlink" title="Search"></a>Search</h1><h2 id="树搜索与图搜索"><a href="#树搜索与图搜索" class="headerlink" title="树搜索与图搜索"></a>树搜索与图搜索</h2><p>Tree Search：不设置vis数组，不考虑节点有无被遍历过</p>
<p><strong>搜索树（search trees）</strong> </p>
<p>对于一个状态出现的次数没有限制。</p>
<p>通过移除一个与部分计划对应的节点（用给定的策略来选择）并用它所有的子节点代替它，我们不断地<strong>扩展expand</strong>我们的边缘。用子节点代缘上的元素,相当于丢弃一个长度为n的计划并考虑所有源于它的长度为（n+1）的计划。我们继续这一操作，直到最终将目标从边缘移除为止。</p>
<p>Graph Search：设置vis数组，每次出堆的时候就标记visited，每次只遍历邻接的unvisited的点</p>
<p><strong>图搜索（graph search）</strong></p>
<p>跟踪哪些状态已经扩展过，确保每一个节点在扩展前不在这个集合中，并且在扩展后将其加入集合里。经过这种优化的树搜索称为<strong>图搜索（graph search）</strong></p>
<h2 id="DFS-Depth-First-Search"><a href="#DFS-Depth-First-Search" class="headerlink" title="DFS(Depth-First Search)"></a>DFS(Depth-First Search)</h2><p>深度优先搜索，故名意思一条边走到底再回去，优先搜索最深的边，用stack实现，头进头出</p>
<p>时间复杂度$O(b^m)$ 空间复杂度$O(bm)$</p>
<p>Complete（完备性）：定义是保证能找到答案在答案存在的情况。深度优先搜索并<strong>不具有完备性</strong>。如果在状态空间图中存在回路，这必然意味着相应搜索树的深度将是无限的。因此，存在这样一种可能性，即DFS老实地在无限大的搜索树中搜索最深的节点而不幸地陷入僵局，注定无法找到解。</p>
<p>Optimal：显然不是最优</p>
<p><img src="/2022/10/29/AI复习总结/1.png" alt=""></p>
<h2 id="BFS-Breadth-First-Search"><a href="#BFS-Breadth-First-Search" class="headerlink" title="BFS(Breadth-First Search)"></a>BFS(Breadth-First Search)</h2><p>广度优先搜索，优先最浅的边，用queue实现，头进尾出</p>
<p>时间复杂度$O(b^s)$，空间复杂度$O(b^s)$，Shallowest点的深度为s</p>
<p>Complete：是完整的搜索</p>
<p>Optimal：只有cost都为1才能找到最优解</p>
<p><img src="/2022/10/29/AI复习总结/2.png" alt=""></p>
<h2 id="Iterative-Deepening-迭代加深的深度优先搜索"><a href="#Iterative-Deepening-迭代加深的深度优先搜索" class="headerlink" title="Iterative Deepening(迭代加深的深度优先搜索)"></a>Iterative Deepening(迭代加深的深度优先搜索)</h2><p><strong>Iterative Deepening</strong></p>
<p><img src="/2022/10/29/AI复习总结/3.png" alt=""></p>
<p>从深度为0开始，深度不断增大，直到找到目标</p>
<p>Complete：Yes</p>
<p>Optimal：Yes</p>
<p>时间复杂度和空间复杂度都是$ O(b^d)$,<strong>比BFS空间更优</strong></p>
<h2 id="UCS-Uniform-Cost-Search"><a href="#UCS-Uniform-Cost-Search" class="headerlink" title="UCS(Uniform Cost Search)"></a>UCS(Uniform Cost Search)</h2><p>优先搜索最便宜的点路径，用的是优先队列</p>
<p>If that solution costs $C^<em>$ and arcs cost(两点间的最小代价) $\epsilon$ , then the“effective depth” is roughly $C^</em>/\epsilon$</p>
<p>时间复杂度$O(C^*/\epsilon)$(exponential in effective depth)</p>
<p>空间复杂度$O(C^*/\epsilon)$</p>
<p>Complete：Yes</p>
<p>Optimal：Yes</p>
<p><img src="/2022/10/29/AI复习总结/4.png" alt=""></p>
<h2 id="GS-Greedy-Search"><a href="#GS-Greedy-Search" class="headerlink" title="GS(Greedy Search)"></a>GS(Greedy Search)</h2><p>Heuristics函数，估计期望函数一般表示到重点的距离估计</p>
<p>贪心直往离终点最近的路走，必然不优。</p>
<p>Complete：Yes</p>
<p>Opitimal：No</p>
<p>如果存在一个目标状态，贪婪搜索<strong>无法保证能找到它</strong>，它也<strong>不是最优的</strong>，尤其是在选择了非常糟糕的启发函数的情况下。在不同场景中。它的行为通常是不可预测的，有可能一路直奔目标状态，也有可能像一个被错误引导的DFS一样并遍历所有的错误区域。</p>
<h2 id="A-Search"><a href="#A-Search" class="headerlink" title="A* Search"></a>A* Search</h2><p>$f(n)=g(n)+h(n)$</p>
<p>结合贪心和UCS，按该点的h+该点到起点的cost</p>
<p>H函数的性质：①admissible：$0\leq h(n)\leq h^*(n)$，小于实际n到重点的距离。</p>
<p>②Consistency：$h(A)-h(C)\leq cost(A,C)$，两个点H只差小于真实距离，<strong>满足此性质即可满足最优解</strong></p>
<p>Consistency $\implies$ admissibility</p>
<h1 id="CSP-Constraint-Satisfaction-Problems"><a href="#CSP-Constraint-Satisfaction-Problems" class="headerlink" title="CSP(Constraint Satisfaction Problems)"></a>CSP(Constraint Satisfaction Problems)</h1><p>Constraint Graphs：图上相连的点表示这两个点之间有限制关系</p>
<h2 id="搜索方法："><a href="#搜索方法：" class="headerlink" title="搜索方法："></a>搜索方法：</h2><p>Backtracking Search：总的来说就是DFS+回溯算法，把所有可以取的值都取试一遍，可以继续就不断往下搜索，遇到问题就返回，不断撤回取值换一个值知道找到解。很慢，会走很多弯路，浪费时间。</p>
<p><img src="/2022/10/29/AI复习总结/5.png" alt=""></p>
<h3 id="改进过程"><a href="#改进过程" class="headerlink" title="改进过程"></a>改进过程</h3><h3 id="Filtering"><a href="#Filtering" class="headerlink" title="Filtering"></a>Filtering</h3><p>每次取值的时候把不能用的过滤掉，减少取值的domain</p>
<p>①Forward checking：每次划掉与当前取值变量有关联的变量取值，缺点是不能提前发现错误，比如在第三步的时候NT和SA都只能取蓝色其实已经矛盾了，因为每次只检查了与现在在填的格子相邻的格子取值，没有单独考虑相邻格子和其限制点的取值比较。</p>
<p><img src="/2022/10/29/AI复习总结/6.png" alt=""></p>
<p>②Constraint Propagation：加入了每次将每个点都和其他相关的限制点domain的比较，加入consitent定义来检查每个点的限制点取值比较</p>
<p>An arc X → Y is consistent iff for every x in the tail there is some y in the head which could be assigned without violating a constraint</p>
<p>A simple form of propagation makes sure all arcs are consistent，每次propagation确保每个点都能满足consistent条件</p>
<p><img src="/2022/10/29/AI复习总结/7.png" alt=""></p>
<p>这里其实我觉得NSW也可以取蓝色，V变成红绿，这里表现能提前排除错误</p>
<p>K-Consistency：弧一致性是一个更广义的一致性概念——<strong>k-相容（k-consistency）</strong> 的子集，它在强制执行时能保证对于CSP中的任意k个节点，对于其中任意k-1个节点组成的子集的赋值都能保证第k个节点能至少有一个满足相容的赋值。这个想法可以通过<strong>强k-相容（strong k-consistency）</strong> 的思想进行进一步拓展。一个具有强k-相容的图拥有这样的性质，任意k个节点的集合不仅是k-相容的，也是k-1, k-2, …,1-相容的。于是，在CSP中更高阶的相容计算的代价也更高。在这一广义的相容的定义下，我们可以看出弧相容等价于2-相容。</p>
<h3 id="Ordering"><a href="#Ordering" class="headerlink" title="Ordering"></a>Ordering</h3><p>①<strong>最小剩余值Minimum Remaining Values (MRV)</strong>：当选择下一个要赋值的<strong>变量</strong>时，用MRV策略能选择有效剩余值最少的未赋值变量（即约束最多的变量）。这很直观，因为受到约束最多的变量也最容易耗尽所有可能的值，并且如果没有赋值的话最终会回溯，所以最好尽早赋值。</p>
<p>这个是针对赋值的variable的选择</p>
<p>②<strong>最少约束值Least Constraining Value (LCV)</strong>：同理，当选择下一个要赋值的<strong>值</strong>时，一个好的策略就是选择从剩余未分配值的域中淘汰掉最少值的那一个。要注意的是，这要求更多的计算（比方说，对每个值重新运行弧相容/前向检测或是其他过滤方法来找到其LCV），但是取决于用途，仍然能获得更快的速度。</p>
<p>这个是针对对每个variable的value如何选择，尽量不要造成更多约束，选择约束最少的value，如下图应选择红色而不是蓝色</p>
<p><img src="/2022/10/29/AI复习总结/8.png" alt=""></p>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><h3 id="Tree-Structured-CSPs"><a href="#Tree-Structured-CSPs" class="headerlink" title="Tree-Structured CSPs"></a>Tree-Structured CSPs</h3><p><img src="/2022/10/29/AI复习总结/9.png" alt=""></p>
<p>Theorem: if the constraint graph has no loops, the CSP can be solved in $O(n d^2 )$ time</p>
<ul>
<li>首先，在CSP的约束图中任选一个节点来作为树的根节点（具体选哪一个并不重要，因为基础图论告诉我们一棵树的任一节点都可以作为根节点）。</li>
<li>将树中的所有无向边转换为指向根节点反方向的有向边。然后将得到的有向无环图<strong>线性化（linearize）</strong>（或<strong>拓扑排序（topologically sort）</strong>）。简单来说，这也就意味着将图的节点排序，让所有边都指向右侧。注意我们选择节点A作为根节点并让所有边都指向A的反方向，这一过程的结果是如下的CSP转换：</li>
</ul>
<p>①Remove backward: For i = n : 2, apply RemoveInconsistent(Parent(Xi ),Xi )，从孩子往父亲搜索，剪掉父亲不能consistent的值</p>
<p>②Assign forward: For i = 1 : n, assign Xi consistently with Parent(Xi )</p>
<p>从父亲开始往孩子赋值，在孩子中选取和父亲consistent的值</p>
<h3 id="Nearly-Tree-Structured-CSPs"><a href="#Nearly-Tree-Structured-CSPs" class="headerlink" title="Nearly Tree-Structured CSPs"></a>Nearly Tree-Structured CSPs</h3><p>通过<strong>割集条件设置（cutset conditioning）</strong>，树形结构算法能推广到树形结构相近的的CSP中。割集条件设置包括首先找到一个约束图中变量的最小子集，这样一来删去他们就能得到一棵树（这样的子集称为<strong>割集（cutset）</strong>）。举个例子，在我们的地图染色例题中，South Australia（SA）是可能的最小的割集：</p>
<p><img src="/2022/10/29/AI复习总结/10.png" alt=""></p>
<p>意思大概是通过不断的割掉点来减少图的边数和点数吧</p>
<h1 id="Adversarial-Search"><a href="#Adversarial-Search" class="headerlink" title="Adversarial Search"></a>Adversarial Search</h1><h2 id="Minimax-Values"><a href="#Minimax-Values" class="headerlink" title="Minimax Values"></a>Minimax Values</h2><p>基本内容就是一个人求最大，另一个人为了妨碍这个人只求最小</p>
<p>由于是本质还是DFS，所以复杂度是$O(b^m)$</p>
<p><img src="/2022/10/29/AI复习总结/11.png" alt=""></p>
<h2 id="α-β剪枝-Alpha-Beta-Pruning"><a href="#α-β剪枝-Alpha-Beta-Pruning" class="headerlink" title="α-β剪枝 Alpha-Beta Pruning"></a>α-β剪枝 Alpha-Beta Pruning</h2><p><img src="/2022/10/29/AI复习总结/12.png" alt=""></p>
<p>简单来说就是对于一个求最大\最小值的父亲节点的子节点，前面已经求出一个孩子的值，由于父亲只会返回最大\最下的孩子节点的值，如果某一个孩子的某一个节点已经相对于前一个孩子的值对父亲没有贡献了，就可以不搜索这个子树了，剪枝掉。</p>
<p>例子：下图f分支已经有b节点的10，由于a求最小，而f的100大于了10，并且e节点本身是要选最大值的，已经找出一个比较大的可能答案且比10还大，说明a节点不可能选e，故g要被剪。同理对于h节点，i这个还是是2且2已经小于了10，h本身求得的最小值已经小于a节点，根节点就不可能选h了所以l就被剪掉了。</p>
<p><img src="/2022/10/29/AI复习总结/13.png" alt=""></p>
<p>时间复杂度：$O(b^{m/2})$</p>
<p><strong>讨论一下什么节点的孩子是可能会被剪掉的</strong></p>
<p>①子树节点的最左边的子树一定不会被剪掉，无论如何都要比较第一个值，所以最左边最左下角的子树都要全部遍历（cd,jk)，同时所有没有被剪得节点的左边第一个孩子一定会遍历，比如上图中的f</p>
<p>②能被剪掉的一定是自己的siblings已经得出了部分答案，自己在被比较的过程中才会因为前一个siblings得出的答案无效所以中止检索。</p>
<h2 id="估计函数-Evaluation-Function"><a href="#估计函数-Evaluation-Function" class="headerlink" title="估计函数 Evaluation Function"></a>估计函数 Evaluation Function</h2><p>一个好的估计函数能给更好的状态赋更高的值。估计函数在<strong>深度限制（depth-limited）minimax</strong>中广泛应用，即将最大可解深度处的非叶子结点都视作叶子节点，然后用仔细挑选的估计函数给其赋虚值。由于估计函数只能用于估计非叶子结点的效益，这使得minimax的最优性不再得到保证。</p>
<p>$Eval(s)=w_1f_1(s)+w_2f_2(s)+ …+w_nf_n(s)$</p>
<p> $f(s)$ 对应从状态s中提取的一个特征，每个特征被赋予了一个相应的权重  。特征就是游戏状态中能够提取并量化的一些元素。</p>
<h2 id="Expectimax-Search"><a href="#Expectimax-Search" class="headerlink" title="Expectimax Search"></a>Expectimax Search</h2><p>Expectimax在游戏树中加入了<strong>机会节点（chance nodes）</strong>，与考虑最坏情况的最小化节点不同，机会节点会考虑<strong>平均情况（average case）</strong>。更准确的说，最小化节点仅仅计算子节点的最小效益，而机会节点计算<strong>期望效益（expected utility）</strong> 或期望值。Expectimax给节点赋值的规则如下：</p>
<p><img src="/2022/10/29/AI复习总结/15.png" alt=""></p>
<p>其中，$p(s’|s)$  要么表示在不能确定操作的情况下从状态s移动到s’的概率，要么表示对手的操作使得最终从s移动到s’的概率，这取决于游戏和游戏树的特性。从这个定义来看，minimax就是expectimax的一种特例，最小化节点就是认为值最小的子节点概率为1而其他子节点概率为0的机会节点。</p>
<p>总的来说，概率是用来合理反映游戏状态的，但我们会在后面的笔记中更详细地介绍这一过程的原理。现在，我们可以吧这些概率看做游戏自带的特性。</p>
<p>Expectimax的伪代码与minimax很相似，就是把最小效益换成了期望效益，这是因为最小化节点被替换成了机会节点：</p>
<p><img src="/2022/10/29/AI复习总结/14.png" alt=""></p>
<p>关于Expectimax，最后再说一句，需要重点注意的是，必须要遍历机会节点的所有子节点——不能再像minimax中一样进行剪枝。与minimax中求最大值或最小值时不同，每个值都会影响expectimax计算的期望值。不过，当我们已知节点有限的取值范围时，剪枝也是有可能的。</p>
<h1 id="Propositional-Logic"><a href="#Propositional-Logic" class="headerlink" title="Propositional Logic"></a>Propositional Logic</h1><p><img src="/2022/10/29/AI复习总结/16.png" alt=""></p>
<h2 id="Conjunctive-Normal-Form-CNF"><a href="#Conjunctive-Normal-Form-CNF" class="headerlink" title="Conjunctive Normal Form (CNF)"></a>Conjunctive Normal Form (CNF)</h2><p>conjunction of disjunctions of literals (clauses)</p>
<p>基本形式是()里面都是∪，外面套∩</p>
<h2 id="Horn-logic"><a href="#Horn-logic" class="headerlink" title="Horn logic"></a>Horn logic</h2><p>Horn logic: only (strict) Horn clauses are allowed</p>
<p>– A Horn clause has the form:</p>
<p>$P_1 \bigcap P_2 \bigcap …P_n \implies Q$</p>
<p>or alternatively</p>
<p>$ \neg P_1 \bigcup \neg  P_2 \bigcup … \neg P_n \bigcup Q$</p>
<p>where Ps and Q are non-negated proposition symbols</p>
<h2 id="经典例题"><a href="#经典例题" class="headerlink" title="经典例题"></a>经典例题</h2><p>$[C \bigcup (\neg A \bigcap \neg B)] = [(\neg A \bigcap \neg B)\bigcup (C \bigcap \neg B) \bigcup ( \neg A \bigcap C) \bigcup C]$</p>
<p>$(\neg A \bigcup \neg B)=((A \bigcap B) \rightarrow (A \bigcap \neg A))$</p>
<h2 id="Validity-and-satisfiability"><a href="#Validity-and-satisfiability" class="headerlink" title="Validity and satisfiability"></a>Validity and satisfiability</h2><p>A sentence is valid if it is true in all models</p>
<p>A sentence is satisfiable if it is true in some model</p>
<p>A sentence is unsatisfiable if it is true in no models</p>
<p>S is valid iff. $\neg $S is unsatisfiable</p>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><h3 id="entails"><a href="#entails" class="headerlink" title="entails"></a>entails</h3><p>$ A| !!!\ = B$ A entails B,相当于$A \implies B$</p>
<p>A is valid if and only if True entails A</p>
<h3 id="proof"><a href="#proof" class="headerlink" title="proof"></a>proof</h3><p>$A | !!!- B$ a demonstration of entailment from A to B</p>
<p>Method 1: model checking</p>
<p>Method 2: application of inference rules</p>
<p><strong>Sound inference</strong>：everything that can be proved is in fact entailed</p>
<p><strong>Complete inference</strong>：everything that is entailed can be proved</p>
<p>Resolution algorithm：Prove$KB |!!!= \alpha$ Proof by contradiction,show $KB |!!!= \alpha$ is unsatisfiable</p>
<ul>
<li><p>把$KB  \bigcap \neg \alpha$转换为CNF</p>
</li>
<li><p>. Repeatedly apply the resolution rule to add new clauses, until one of the two things happens</p>
<p>a) Two clauses resolve to yield the empty clause, in which</p>
<p>case$KB |!!! = \alpha $</p>
<p>b) There is no new clause that can be added, in which case</p>
<p>KB does not entail α</p>
</li>
</ul>
<h2 id="Forward-chaining"><a href="#Forward-chaining" class="headerlink" title="Forward chaining"></a>Forward chaining</h2><p>从 implies 前面的条件开始往implies后面的值推导</p>
<p><img src="/2022/10/29/AI复习总结/17.png" alt=""></p>
<p>从A B L M  P Q ，没往上推 节点的未知数就-1，直到都为0</p>
<h2 id="Backward-chaining"><a href="#Backward-chaining" class="headerlink" title="Backward chaining"></a>Backward chaining</h2><p>从果找因</p>
<p>• Idea: work backwards from the query q:</p>
<p>– to prove q by BC</p>
<p>• check if q is known to be true already, or prove by BC all premises of some rule concluding q</p>
<p>• Avoid loops: check if new subgoal is already on the goal</p>
<p>stack</p>
<p>• Avoid repeated work: check if new subgoal</p>
<ol>
<li><p>has already been proved true, or</p>
</li>
<li><p>has already failed</p>
</li>
</ol>
<h1 id="First-Order-Logic"><a href="#First-Order-Logic" class="headerlink" title="First-Order Logic"></a>First-Order Logic</h1><h2 id="FOL"><a href="#FOL" class="headerlink" title="FOL"></a>FOL</h2><p><img src="/2022/10/29/AI复习总结/18.png" alt=""></p>
<p><strong>In a FOL sentence, every variable must be bound.</strong></p>
<p>判断是否为FOL句式，要看变量有没有被量词限定</p>
<h2 id="Atomic-sentences"><a href="#Atomic-sentences" class="headerlink" title="Atomic sentences"></a>Atomic sentences</h2><p><img src="/2022/10/29/AI复习总结/19.png" alt=""></p>
<p>复杂句子由很多原子句子组成</p>
<ul>
<li>任意句式：$\forall x, At(x,stu) \implies Smart(x)$(Everyone at ShanghaiTech is smart)</li>
</ul>
<p>一般任意都接imply，不要用并</p>
<ul>
<li>存在句式：$\exist x,At(x,stu) \implies Smart(x)$ (Someone at ShanghaiTech is smart)</li>
</ul>
<p>存在句式用并，不要用imply</p>
<p><strong>任意和存在量词的一些性质</strong></p>
<p><img src="/2022/10/29/AI复习总结/20.png" alt=""></p>
<h2 id="两个变量替换原则"><a href="#两个变量替换原则" class="headerlink" title="两个变量替换原则"></a>两个变量替换原则</h2><ul>
<li>Universal instantiation (UI)</li>
</ul>
<p>全称列举规则，UI 规则的实质，可以看成是包含量词的符号式的展开式，通过简化式推论规则，缩略为其中某个“合取项”。</p>
<p>大概是变量代入具体的意思（），变量替换</p>
<ul>
<li>Existential Instantiation(EI)</li>
</ul>
<p>存在列举规则</p>
<p>存在列举规则，就是摘除存在量词。</p>
<p>与 UI 不同， EI 规则摘除量词后，该行中的自由变量不是全称，而是被制定出来用以指派一个具体个体的名称。</p>
<p>请注意，这个个体不是被任意选出的。</p>
<h2 id="Unification"><a href="#Unification" class="headerlink" title="Unification"></a>Unification</h2><p>好像就是找出一个变量替换的方式。。。如下图找到x,y分别代表什么</p>
<p><img src="/2022/10/29/AI复习总结/21.png" alt=""></p>
<h2 id="Forward-chaining-1"><a href="#Forward-chaining-1" class="headerlink" title="Forward chaining"></a>Forward chaining</h2><p>和之前那个Forward chaining差不多，从implies前的推到implies后面的</p>
<p>性质：</p>
<ul>
<li><p>Sound and complete for first-order Horn clauses</p>
</li>
<li><p>FC terminates for first-order Horn clauses with no functions (Datalog) in finite number of iterations</p>
</li>
<li><p>In general, FC may not terminate if α is not entailed，只要没找到不会中止</p>
</li>
</ul>
<h2 id="Backward-chaining-1"><a href="#Backward-chaining-1" class="headerlink" title="Backward chaining"></a>Backward chaining</h2><p>和之前那个Back chaining差不多，从implies后的找到implies前面的，由果及因</p>
<p>性质：</p>
<ul>
<li><p>Depth-first recursive proof search: space is linear in size of proof线性证明</p>
</li>
<li><p>Avoid infinite loops by checking current goal against every goal on stack有效避免无限循环</p>
</li>
<li><p>Avoid repeated subgoals by caching previous results不会重复找子问题</p>
</li>
</ul>
<p><strong>FC and BC are sound and complete with Horn clauses and run linear in space and time.</strong></p>
<h1 id="Bayes-Network"><a href="#Bayes-Network" class="headerlink" title="Bayes Network"></a>Bayes Network</h1><h2 id="条件独立"><a href="#条件独立" class="headerlink" title="条件独立"></a>条件独立</h2><p>$X \perp !!! \perp Y|Z$ : X is conditionally independent of Y given Z</p>
<p>$X \perp !!! \perp Y|Z \Leftrightarrow P(x|y,z)=P(x|z),P(x,y|z)=P(x|z)P(y|z)$</p>
<p>贝叶斯网络是一种描述<strong>随机变量之间互相条件独立关系的有向无环图</strong>。在这个有向无环图中，每个节点代表一个<strong>随机变量对其父节点的条件概率分布</strong> $P(X_i|parents(X_i)$ ，每一条边可以理解成变量之间的联系。</p>
<p>A Bayes net = Topology (graph) + Local Conditional Probabilities</p>
<p>贝叶斯网络的空间：n变量，最大取值空间d，最大父节点数量k，满节点的分布$O(d^n)$贝叶斯网络大小$O(nd^{k+1})$</p>
<p>贝叶斯网络是有向无环图，$B \rightarrow A$ 代表B是A的条件，有公式</p>
<p>$P(X_1,…,X_n)=\prod_{i}P(X_i|Parents(X_i)$</p>
<p><img src="/2022/10/29/AI复习总结/22.png" alt=""></p>
<p>When Bayes nets reflect the true causal patterns:</p>
<p>▪ Often simpler (fewer parents, fewer parameters)</p>
<p>▪ Often easier to assess probabilities</p>
<p>▪ Often more robust: e.g., changes in frequency of burglaries should not affect the rest of the model</p>
<p>BNs need not actually be causal</p>
<p><strong>BN don’t need to reflect the true causal patterns</strong></p>
<h3 id="给定贝叶斯网络上部分点，判断两点是否独立："><a href="#给定贝叶斯网络上部分点，判断两点是否独立：" class="headerlink" title="给定贝叶斯网络上部分点，判断两点是否独立："></a>给定贝叶斯网络上部分点，判断两点是否独立：</h3><p><strong>[Step 1]. Draw the ancestral graph.</strong></p>
<p>根据原始概率图，构建包括表达式中包含的变量以及这些变量的ancestor节点（父节点、父节点的父节点…）的图。<strong>注意不能包括孩子节点，不然会引起错误。</strong></p>
<p><strong>[Step 2]. “Moralize” the ancestral graph by “marrying” the parents.</strong></p>
<p>连接图中每个collider结构中的父节点，即若两个节点有同一个子节点，则连接这两个节点。（若一个变量的节点有多个父节点，则分别链接每一对父节点）。</p>
<p><strong>[Step 3]. “Disorient” the graph by replacing the directed edges (arrows) with undirected edges (lines).</strong></p>
<p>去掉图中所有的路径方向，将directional graph变为non-directional graph。</p>
<p><strong>[Step 4]. Delete the givens and their edges.</strong></p>
<p>从图中删除需要判断的概率表达式中作为条件的变量，以及和他们相连的路径。比如“是否P(A|BDF) = P(A|DF)?”，我们删掉D, F变量以及他们的路径。</p>
<p><strong>[Step 5]. Read the answer off the graph.</strong></p>
<ul>
<li>如果变量之间没有连接，则它们在给定条件下是独立的；</li>
<li>如果变量之间有路径连接，则它们不能保证是独立的（或者粗略地说他们是不独立的，基于概率图来说）；</li>
<li>如果其中一个变量或者两者都不包含在现在的图中（作为观测条件，在step 4 被删掉了），那么他们是独立的。</li>
</ul>
<p>补充概念：活跃与非活跃道路</p>
<p><img src="/2022/10/29/AI复习总结/23.png" alt=""></p>
<h2 id="马尔可夫网络-Markov-Networks"><a href="#马尔可夫网络-Markov-Networks" class="headerlink" title="马尔可夫网络(Markov Networks)"></a>马尔可夫网络(Markov Networks)</h2><p>是一个无向图</p>
<p>Markov network = undirected graph + potential functions</p>
<h3 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h3><p><img src="/2022/10/29/AI复习总结/24.png" alt=""></p>
<p>有一个特定的rule$P(X_1,X_2,…,X_t)=P(X_1)\prod_{t=2}^TP(X_t|X_{t-1})$</p>
<p>只需要初始概率和条件概率表（CPT）就可以得出任何一个时段，某一件事所要发生的概率。如果一个马尔可夫链接近无限长，最终概率会越来越接近一个平稳大小。</p>
<p>这个我们称作，静态分布（Stationary Distributions)。每一个马尔可夫链都至少有一个静态分布。对于一些特殊结构的马尔可夫链，可能只有唯一一个静态分布。比如：不可还原的（Irreducible）马尔可夫链或者叫正常的（Regular)马尔可夫链</p>
<p>表示的是马尔可夫链上的点可以有机会到达其它任何一个点，就表示它是Irreducible。</p>
<p>当一个马尔可夫链是irreducible的，那么它一定会有唯一一个静止状态。</p>
<p>但要想确定它是否可能最终收敛到这个静止状态，还需要另外一个属性来判断，看他是不是非周期性的（Aperiodic）</p>
<h3 id="隐马尔可夫模型-HMM"><a href="#隐马尔可夫模型-HMM" class="headerlink" title="隐马尔可夫模型(HMM)"></a>隐马尔可夫模型(HMM)</h3><p>马尔可夫链的基础上加了一层可观察层，原来的链路变成了不可观察：</p>
<p><img src="/2022/10/29/AI复习总结/25.png" alt=""></p>
<p>$P(X_1,E_1,…,X_T,E_T)=P(X_1)P(E_1|X_1)\prod_{t=2}^TP(X_t|X_{t-1})P(E_t|X_t)$</p>
<p>性质：</p>
<ol>
<li>在已知现在(present)的可观察的状态下，未来(future)的可观察的状态，是独立于过去(past)的可观察的状态。</li>
<li>在已知所有的可观察的状态下，可观察的状态与可观察状态之间是独立的。</li>
</ol>
<h2 id="马尔可夫网络-Markov-Nets"><a href="#马尔可夫网络-Markov-Nets" class="headerlink" title="马尔可夫网络(Markov Nets)"></a>马尔可夫网络(Markov Nets)</h2><p>对于马尔可夫网络中，我们也可以用势能 （$\phi_i$ ）来表示不同结点间影响力的大小或者结点团之间影响力的大小。其实这个概念是源自物理中的势能。</p>
<p>我们可以通过一个被称作联合分布函数的来表示这个马尔可夫网络（也被称作吉布斯分布/Gibbis分布）</p>
<p>引入一个clique（小集团）的定义，用这个来定义分布</p>
<p>$P(X)=\frac{1}{Z}\prod_{c\in cliques(H)}\phi_c(x_c)$</p>
<p>$Z=\sum_x \prod_{c \in cliques(H)}\phi_c(x_c)$</p>
<p><img src="/2022/10/29/AI复习总结/27.png" alt=""></p>
<p><img src="/2022/10/29/AI复习总结/26.png" alt=""></p>
<p>$P(X)=\frac{1}{Z}\phi_1(A,B)\phi_2(B,C)\phi_3(C,D)\phi_4(D,A)$</p>
<h2 id="马尔可夫毯"><a href="#马尔可夫毯" class="headerlink" title="马尔可夫毯"></a>马尔可夫毯</h2><p>简单来说就是T的父亲节点+T的孩子+T的孩子的父亲</p>
<p><img src="/2022/10/29/AI复习总结/28.png" alt=""></p>
<h3 id="贝叶斯网络和马尔可夫网络的转换"><a href="#贝叶斯网络和马尔可夫网络的转换" class="headerlink" title="贝叶斯网络和马尔可夫网络的转换"></a>贝叶斯网络和马尔可夫网络的转换</h3><p>Steps</p>
<ol>
<li><p>Moralization</p>
</li>
<li><p>Construct potential functions from CPTs</p>
</li>
</ol>
<p>The BN and MN encode the same distribution，两个表示同一个分布</p>
<p>But they don’t encode the same set of conditional independence但表示的独立性不同</p>
<p>①链式结构，直接转换</p>
<p><img src="/2022/10/29/AI复习总结/29.png" alt=""></p>
<p>②非链式结构，要加边</p>
<p>只要有共同的父亲就要练边，每个父亲两两相连</p>
<p><img src="/2022/10/29/AI复习总结/30.png" alt=""></p>
<h1 id="Bayes-Nets-Exact-Inference"><a href="#Bayes-Nets-Exact-Inference" class="headerlink" title="Bayes Nets: Exact Inference"></a>Bayes Nets: Exact Inference</h1><p>calculating some useful quantity from a probabilistic model (joint probability distribution)</p>
<h2 id="Join-factors"><a href="#Join-factors" class="headerlink" title="Join factors"></a>Join factors</h2><p>字面意思就是把条件放到条件前面去$P(r) * P(t|r) \rightarrow P(r,t)$</p>
<p>需要一直单独的概率和条件概率</p>
<p><img src="/2022/10/29/AI复习总结/31.png" alt=""></p>
<h2 id="Eliminate-factors"><a href="#Eliminate-factors" class="headerlink" title="Eliminate factors"></a>Eliminate factors</h2><p>直接把某个变量删掉，必须知道联合分布</p>
<p>一般采用enumeration 的顺序，即从条件到非条件，沿着箭头来，但也可以不按这个顺序来。</p>
<p>每次eliminate一个变量就相当于把他哦才能够贝叶斯网络里面划掉，只用eliminate目标变量的祖先节点，不用管孩子节点。</p>
<p><img src="/2022/10/29/AI复习总结/32.png" alt=""></p>
<h1 id="Bayes-Nets-Approximate-Inference"><a href="#Bayes-Nets-Approximate-Inference" class="headerlink" title="Bayes Nets: Approximate Inference"></a>Bayes Nets: Approximate Inference</h1><h2 id="Prior-Sampling-直接采样"><a href="#Prior-Sampling-直接采样" class="headerlink" title="Prior Sampling (直接采样)"></a>Prior Sampling (直接采样)</h2><p>生成全联合概率分布，这里可以理解为路上的随机采访，因为每个节点的值概率是不一样的，所以在每个节点采样值和概率相关，我们就是通过直接采样，就像街上的直接采访一样，一个一个采样，最后生成一个概率分布表。</p>
<p>直接按原图的顺序一个个采就行</p>
<p><img src="/2022/10/29/AI复习总结/33.png" alt=""></p>
<h2 id="Rejection-Sampling-拒绝采样"><a href="#Rejection-Sampling-拒绝采样" class="headerlink" title="Rejection Sampling(拒绝采样)"></a>Rejection Sampling(拒绝采样)</h2><p>在直接采样的基础上删去不符合的样本</p>
<p>对于含有随机接受度的拒绝采样，如果采样被拒绝了即取负值，则在下一个接受度重新取直到是正的再继续取别的。只要被拒绝就重新采样。</p>
<p><img src="/2022/10/29/AI复习总结/34.png" alt=""></p>
<h2 id="Likelihood-Weighting-似然加权"><a href="#Likelihood-Weighting-似然加权" class="headerlink" title="Likelihood Weighting(似然加权)"></a>Likelihood Weighting(似然加权)</h2><p>对<strong>证据变量不采样</strong>，直接利用条件分布表的概率，给每个样本一个权重，也称作似然。</p>
<p>我的理解就是weight就是证据变量的条件概率乘积下图的意思就是采样S,W是证据变量，其他都要有一个采样这里sample是c,s,r,w，c,r都是采样的</p>
<p>所以该样本weight就等于$P(S|C)P(W|S,R)$，这个1.0指的是weight一开始就是1.0给我干寂寞了之前一直没看懂</p>
<p>weight是evidence的$e_i,\prod_i^mP(e_i|Parents(e_i))$</p>
<p><img src="/2022/10/29/AI复习总结/35.png" alt=""></p>
<p>下图写的就是这个过程</p>
<p><img src="/2022/10/29/AI复习总结/36.png" alt=""></p>
<p>下面就是计算完所有权值之后再来算给定的条件概率，对比普通的采样是直接根据有多少个才看作是w，这个是每个w不同</p>
<p><img src="/2022/10/29/AI复习总结/37.png" alt=""></p>
<p>还有一种给你随机序列让你完成一组采样和weight计算的类型</p>
<p>estimate$P(C=1|B=1,E=1)$，已知B,E为evidence，要采样剩余的A,C,D</p>
<p>采样标准：select a value <img src="http://latex.codecogs.com/svg.latex?a" alt=""> from the table, and choose $W=1$if$a \geq P(W=0) $</p>
<p>根据第一个值0.249&gt;0.2,A=1,0.052&lt;0.6,C=0,0,299&lt;0.6 D=0</p>
<p>故A=1,C=0,D=0</p>
<p>$weight=P(B|A)P(\neg C,\neg D|E)=0.8*0.8=0.64$<br><img src="/2022/10/29/AI复习总结/40.png" alt=""></p>
<p><img src="/2022/10/29/AI复习总结/39.png" alt=""></p>
<h2 id="Gibbs-Sampling-吉布斯采样"><a href="#Gibbs-Sampling-吉布斯采样" class="headerlink" title="Gibbs Sampling(吉布斯采样)"></a>Gibbs Sampling(吉布斯采样)</h2><p>先确定一个evidence，已经有一个固定值了，然后随机初始化其他的所有变量，之后随便选一个不是evidence I的变量进行采样，现要求这个I的马尔可夫毯的条件概率如下图就是</p>
<p>先求weight$w(s|c,r,\neg w)=P(s|c)P(\neg w|s,r),w(\neg s|c,r,\neg w)=P(\neg s|c)P(\neg w|\neg s,r)$</p>
<p>再求$p(s|c,r,\neg w)=p(s|c,r,\neg w)/(p(s|c,r,\neg w)+p(\neg s|c,r,\neg w))$</p>
<p>$p(\neg s|c,r,\neg w)=p(\neg s|c,r,\neg w)/(p(s|c,r,\neg w)+p(\neg s|c,r,\neg w))$</p>
<p>一般来说是直接算完就行了，可能会给一个接受率然后根据这个来决定sample T还是F，可能会给一个随机值r让$r \leq p(s|c,r,\neg w))$就取T，不然就是F</p>
<p><img src="/2022/10/29/AI复习总结/38.png" alt=""></p>
<h2 id="MCMC-Markov-Chain-Monte-Carlo"><a href="#MCMC-Markov-Chain-Monte-Carlo" class="headerlink" title="MCMC(Markov Chain Monte Carlo)"></a>MCMC(Markov Chain Monte Carlo)</h2><p>Markov chain = a sequence of randomly chosen states (“random walk”), where each state is chosen conditioned on the previous state</p>
<p>MCMC = sampling by constructing a Markov chain</p>
<p>基本思路：在随机变量x的状态空间S上定义一个满足遍历定理的马尔可夫链$X=x_0,x_1,…,x_t,….$，使得平稳分布就是抽样的目标分布$p(x)p ( x ) 。然后在这个马尔可夫链上开始随机游走，每个时刻得到一个样本。根据遍历定理，当时间趋于无穷时，样本的分布是趋于平稳分布的，样本的函数均值趋于函数的期望均值。所以说当时间足够长时(比如说当n&gt;m时)(从时刻1到m我们称之为燃烧期)，在m时刻之后的时间里随机游走得到的样本值就是对目标分布抽样的结果，得到的函数均值就是近似函数的数学期望。</p>
<p>马尔可夫蒙特卡罗法基本步骤：</p>
<ul>
<li><p>首先，在随机变量x的状态空间S上构造一个满足遍历定理的马尔科夫链，使其平稳分布为目标分布$p(x)$。</p>
</li>
<li><p>从状态空间中的某一点开始出发，用构造的马尔科夫链进行随机游走，产生样本序列$x_0,x_1,…,x_t$。</p>
</li>
<li><p>确定正整数m和n，得到样本值集合$x_{m+1},…,x_n$。求的函数$f(x)$的均值：$E_{p(x)}[f(x)]=\frac{1}{n-m}\sum_{i=m+1}^nf(x_i)$</p>
</li>
</ul>
<h3 id="Metropolis-Hastings"><a href="#Metropolis-Hastings" class="headerlink" title="Metropolis-Hastings"></a>Metropolis-Hastings</h3><p>首先先定义采样时刻t-1的采样值为x，t时刻的采样值为x’，所以对于要抽样的概率分布$p(x)$采用转移核为$p(x,x’)$的马尔科夫链：$p(x,x’)=q(x,x’)\alpha(x,x’)$</p>
<p>其中$q(x,x’)$和$\alpha(x,x’)$分布被称之为提议分布和接受率。而提议分布是另一个马尔可夫链的转移核，是一个比较容易抽样的分布。而接受分布$\alpha(x,x’)$是：$\alpha(x,x’)=min(1,\frac{p(x’)q(x’,x)}{p(x)q(x,x’)})$</p>
<ul>
<li><p>任意选一个初始值$x_0$</p>
</li>
<li><p>对于$i=1,2,…,n$ 循环操作 ①对于$x_{i-1}=x$按分布$q(x,x’)$ 随机抽取一个候选状态$x’$②计算$\alpha(x,x’)$③在(0,1)中去一个随机值如果$\alpha(x,x’)\geq u$<br>则$x_i=x’$否则$x_i=x$</p>
</li>
<li><p>最后得到样本合集$x_{m+1},…,x_n$，算$E_{p(x)}[f(x)]=\frac{1}{n-m}\sum_{i=m+1}^nf(x_i)$</p>
</li>
</ul>
<p>吉布斯采样就是接受值一直为1的特例，一直接受新采样的样本</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 valine -->
<div id="comment">
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#comment' ,
        notify: false,
        verify: false,
        app_id: 'fUtpOauFo2JhcWoBFEnnppJW-gzGzoHsz',
        app_key: 'gFTwG42ACHuyrmwhMfgxcRQQ',
        placeholder: '留下你的评论吧~~',
        pageSize: '10',
        avatar: '',
        avatar_cdn: 'https://gravatar.loli.net/avatar/'
    });
</script>
</div>
<style>
   #comment{
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
            	<span id="busuanzi_container_site_pv">2022总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"left","width":125,"height":250},"mobile":{"show":false},"log":false});</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"left","width":125,"height":250},"mobile":{"show":false},"log":false});</script></body>




 	

    </script>
</html>
